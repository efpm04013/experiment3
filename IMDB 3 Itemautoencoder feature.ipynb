{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/home/kirana/Documents/phd/exp3_autoencoder'\n",
    "DATAPATH='/home/kirana/Documents/final_dissertation_final/experiments/datasets/ml-latest-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df, df_train,df_valid,df,df_ratings,dfflagtrain,dfflagvalid,idx_to_user,\\\n",
    "             idx_to_movie,movie_to_idx,user_to_idx]=pickle.load(open(f'{DATAPATH}/reads.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings=df.pivot(index='movieId',columns='userId',values='rating')\n",
    "df_ratings.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfflagtrain=df.pivot(index='movieId',columns='userId',values='dstype_random_train')\n",
    "dfflagtrain.head()\n",
    "dfflagtrain.fillna(0,inplace=True)\n",
    "df_train=df_ratings*dfflagtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfflagvalid=df.pivot(index='movieId',columns='userId',values='dstype_random_valid')\n",
    "dfflagvalid.head()\n",
    "dfflagvalid.fillna(0,inplace=True)\n",
    "df_valid=df_ratings*dfflagvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencdata (Dataset):\n",
    "    def __init__(self,dfX,dfXv):\n",
    "        self.dfX,self.dfXv=dfX,dfXv\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.dfX.shape[0]\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        return torch.FloatTensor(self.dfX.iloc[idx].values),torch.FloatTensor(self.dfXv.iloc[idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=autoencdata(df_train, df_valid)\n",
    "#dsvalid=autoencdata(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader object\n",
    "dltrain=DataLoader(dstrain,batch_size=bs,shuffle=False)\n",
    "#dlvalid=DataLoader(dsvalid,batch_size=bs,shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 5.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].min(),df['rating'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9724, 610)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model Architecture for the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(x,y,dropout,activation=nn.Sigmoid()):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(x, y),\n",
    "        activation,\n",
    "        nn.Dropout(p=dropout)\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder (nn.Module):    \n",
    "    def __init__(self,n_inp,hidden=[50,10],dropouts=[0,0,0],rating_range=[0.5,5]):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.hidden,self.dropouts,self.rating_range=n_inp,hidden,dropouts,rating_range\n",
    "        encoder=[hidden_layer(n_inp if i==0 else hidden[i-1],hidden[i],dropouts[i],\\\n",
    "                              nn.Sigmoid() if i<len(hidden)-1 else nn.Tanh()) for i in range(len(hidden))]\n",
    "        self.encoder=nn.Sequential(*encoder)\n",
    "        hidden=hidden[::-1]\n",
    "        num_steps=len(hidden)-1\n",
    "        dropouts=dropouts[num_steps:]\n",
    "        decoder=[hidden_layer(hidden[i],hidden[i+1] if i<len(hidden)-1 else n_inp,dropouts[i]) for i in range(len(hidden)-1)]\n",
    "        self.decoder=nn.Sequential(*decoder)\n",
    "        self.fc=nn.Linear(hidden[-1],n_inp)\n",
    "        self.initialize()\n",
    "        self.criterion=nn.MSELoss()\n",
    "    \n",
    "    def initialize(self):\n",
    "        for x in self.encoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "        for x in self.decoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "\n",
    "    def forward (self,Xb):\n",
    "        \n",
    "        encoded=self.encoder(Xb)\n",
    "        decoded=self.decoder(encoded)\n",
    "        out=self.fc(decoded)\n",
    "        outv=out.clone()\n",
    "        out[Xb==0]=0\n",
    "        loss=self.criterion(out,Xb)\n",
    "        return outv,loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[50,10],[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=610, out_features=50, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=50, out_features=610, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0000, 0.0000, 0.0000,  ..., 2.5000, 3.0000, 5.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 2.0000, 0.0000, 0.0000],\n",
      "        [4.0000, 0.0000, 0.0000,  ..., 2.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [3.0000, 0.0000, 0.0000,  ..., 3.0000, 0.0000, 4.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for Xb, Xb_v in dltrain:\n",
    "    print (Xb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 610])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0000, 0.0000, 0.0000,  ..., 2.5000, 3.0000, 5.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 2.0000, 0.0000, 0.0000],\n",
       "        [4.0000, 0.0000, 0.0000,  ..., 2.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [3.0000, 0.0000, 0.0000,  ..., 3.0000, 0.0000, 4.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,loss,preds_train=autoenc.forward(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 610])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.MSELoss"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-4\n",
    "#wd=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "#optimizer=torch.optim.SGD(model_sentiment.parameters(),lr=1e-2,momentum=0.9, weight_decay=wd)\n",
    "metric_fn=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dltrain.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None,\\\n",
    "                 cycle_mult=0,lr_decay=0.7,wd_mult=6,start_lr=2e-2, end_lr=5e-4):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "        self.cycle_mult,self.lr_decay=cycle_mult,lr_decay\n",
    "        self.wd_mult=wd_mult\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            self.start_lr=param_group['lr']\n",
    "            self.start_wd=param_group['weight_decay']\n",
    "        self.wd=self.start_wd\n",
    "        self.lr=self.start_lr\n",
    "        self.end_lr=end_lr\n",
    "        self.n_epoch=0\n",
    "        self.lrs=[1e-2,5e-3,1e-4,5e-4]\n",
    "        self.preds,self.preds_valid,self.trainY,self.actual=[],[],[],[]\n",
    "        self.ratio=self.end_lr/self.start_lr\n",
    "        self.num_steps=self.cycle_mult\n",
    "        self.reset_cycle=self.cycle_mult\n",
    "        \n",
    "    def fit (self,Xb,Xb_v,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        \n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        preds,loss,preds_train=self.model(Xb)\n",
    "        # denominator is the average of the error with non-zero ratings\n",
    "\n",
    "        mean_corrector = Xb.size(0)*Xb.size(1)/(torch.sum(Xb > 0).float() + 1e-10)\n",
    "        mean_corrector_v = Xb_v.size(0)*Xb_v.size(1)/(torch.sum(Xb_v > 0).float() + 1e-10)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds[Xb_v==0]=0\n",
    "            loss_v=self.model.criterion(preds,Xb_v)\n",
    "            \n",
    "            if self.metric_fn is not None:\n",
    "                acc=self.metric_fn(preds,Yb.view(-1),self.device)\n",
    "                acc=acc.item()\n",
    "\n",
    "                if 1==0:\n",
    "                    if mode_train:\n",
    "                        self.trainY.append(Yb.view(-1))\n",
    "                        self.preds.append(preds.data)\n",
    "                    else:\n",
    "                        self.actual.append(Yb.view(-1))\n",
    "                        self.preds_valid.append(preds.data)\n",
    "            else:\n",
    "                acc=0\n",
    "                acc_v=0\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            if 1==0:\n",
    "                lr =self.lrs[torch.randint(0,4,(1,))]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=torch.sqrt(loss.item()*mean_corrector)\n",
    "        myloss_v=torch.sqrt(loss_v.item()*mean_corrector_v)\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)\n",
    "        \n",
    "        return myloss, acc,myloss_v,acc_v\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        epoch_loss_v,epoch_acc_v=0,0\n",
    "\n",
    "        for Xb,Xb_v in iterator:\n",
    "            Xb=Xb.to(self.device)\n",
    "            Xb_v=Xb_v.to(self.device)\n",
    "            #Xb=Xb.squeeze(0)\n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc,loss_v,acc_v=self.fit(Xb,Xb_v,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            epoch_loss_v+=loss_v\n",
    "            epoch_acc_v+=acc_v\n",
    "            \n",
    "            k=k+1\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)} {epoch_loss_v/(k)} ')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/len(iterator)\n",
    "        epoch_acc=epoch_acc/len(iterator)\n",
    "        epoch_loss_v=epoch_loss_v/len(iterator)\n",
    "        epoch_acc_v=epoch_acc_v/len(iterator)\n",
    "            \n",
    "        return epoch_loss,epoch_acc,epoch_loss_v,epoch_acc_v\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1,ylim=None,xlim=None):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "        if xlim is not None:\n",
    "            plt.xlim(xlim)\n",
    "\n",
    "     \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        for epoch in range(n_epochs):                \n",
    "\n",
    "            loss,acc,lossv,accv=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Learning rate {self.lr} Weight Decay {self.wd} Train Loss:{loss}  Valid Loss:{lossv} ')\n",
    "  \n",
    "            if self.cycle_mult:\n",
    "                if self.n_epoch==self.reset_cycle:\n",
    "                    self.lr=self.start_lr\n",
    "                    #self.wd=self.start_wd\n",
    "                    self.reset_cycle*=self.cycle_mult\n",
    "                    #reset_cycle=self.n_epoch+reset_cycle\n",
    "                    self.n_epoch=0\n",
    "                    self.ratio=self.end_lr/self.start_lr\n",
    "                    self.num_steps=self.reset_cycle\n",
    "                else:\n",
    "                    #self.lr*=(self.lr_decay**self.n_epoch)  \n",
    "                    #if self.n_epoch>1:\n",
    "                    #    self.wd*=self.wd_mult\n",
    "                    self.lr=self.lr*(self.end_lr/self.start_lr)**(1/self.num_steps)\n",
    "                    self.n_epoch+=1\n",
    "        \n",
    "\n",
    "                \n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr']=self.lr\n",
    "                #param_group['weight_decay']=self.wd\n",
    "          \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[20,10],[0.6,0.6,0]).to(device)\n",
    "wd=1e-7\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=610, out_features=20, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=20, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=20, out_features=610, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(1e-4,1e-1,dltrain,len(dltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plot_lrs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[50,10],[0.25,0.25,0]).to(device)\n",
    "wd=1e-7\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=3e-2,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=2,start_lr=3e-2,end_lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.1161539554595947  Valid Loss:1.115892767906189 \n",
      "Epoch:1 Learning rate 0.0038729833462074164 Weight Decay 1e-07 Train Loss:1.0356186628341675  Valid Loss:1.0415438413619995 \n",
      "Epoch:2 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:0.9956214427947998  Valid Loss:1.022308111190796 \n",
      "Epoch:3 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.0665830373764038  Valid Loss:1.058861255645752 \n",
      "Epoch:4 Learning rate 0.010779123358892525 Weight Decay 1e-07 Train Loss:1.0262126922607422  Valid Loss:1.0235148668289185 \n",
      "Epoch:5 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:1.0000934600830078  Valid Loss:1.011268973350525 \n",
      "Epoch:6 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.9661991000175476  Valid Loss:0.9827762246131897 \n",
      "Epoch:7 Learning rate 0.0004999999999999998 Weight Decay 1e-07 Train Loss:0.9558252096176147  Valid Loss:0.9680547118186951 \n",
      "Epoch:8 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.0282260179519653  Valid Loss:1.0250827074050903 \n",
      "Epoch:9 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:0.9938983917236328  Valid Loss:1.0014365911483765 \n",
      "Epoch:10 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:0.9551340341567993  Valid Loss:0.960887610912323 \n",
      "Epoch:11 Learning rate 0.006461220105808663 Weight Decay 1e-07 Train Loss:0.9245837330818176  Valid Loss:0.9419009685516357 \n",
      "Epoch:12 Learning rate 0.0038729833462074173 Weight Decay 1e-07 Train Loss:0.9041923880577087  Valid Loss:0.9325555562973022 \n",
      "Epoch:13 Learning rate 0.0023215429523156072 Weight Decay 1e-07 Train Loss:0.8999686241149902  Valid Loss:0.9284330010414124 \n",
      "Epoch:14 Learning rate 0.0013915788418568708 Weight Decay 1e-07 Train Loss:0.8922931551933289  Valid Loss:0.9263750314712524 \n",
      "Epoch:15 Learning rate 0.0008341399288659162 Weight Decay 1e-07 Train Loss:0.8933840394020081  Valid Loss:0.9276701211929321 \n",
      "Epoch:16 Learning rate 0.0005000000000000002 Weight Decay 1e-07 Train Loss:0.889888346195221  Valid Loss:0.9243725538253784 \n",
      "Epoch:17 Learning rate 0.03 Weight Decay 1e-07 Train Loss:0.9539622068405151  Valid Loss:0.9698107838630676 \n",
      "Epoch:18 Learning rate 0.02322666208281794 Weight Decay 1e-07 Train Loss:0.9315540790557861  Valid Loss:0.9344679117202759 \n",
      "Epoch:19 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:0.9078838229179382  Valid Loss:0.921983540058136 \n",
      "Epoch:20 Learning rate 0.013922521437378356 Weight Decay 1e-07 Train Loss:0.9018451571464539  Valid Loss:0.920954167842865 \n",
      "Epoch:21 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:0.8843003511428833  Valid Loss:0.9151511788368225 \n",
      "Epoch:22 Learning rate 0.00834543519353354 Weight Decay 1e-07 Train Loss:0.8808082938194275  Valid Loss:0.9123139977455139 \n",
      "Epoch:23 Learning rate 0.006461220105808662 Weight Decay 1e-07 Train Loss:0.8759106397628784  Valid Loss:0.9037925601005554 \n",
      "Epoch:24 Learning rate 0.005002419201344232 Weight Decay 1e-07 Train Loss:0.8700150847434998  Valid Loss:0.9069645404815674 \n",
      "Epoch:25 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:0.8674905896186829  Valid Loss:0.9098749756813049 \n",
      "Epoch:26 Learning rate 0.002998549181158038 Weight Decay 1e-07 Train Loss:0.8662775754928589  Valid Loss:0.9099286794662476 \n",
      "Epoch:27 Learning rate 0.002321542952315606 Weight Decay 1e-07 Train Loss:0.8652135133743286  Valid Loss:0.9085330963134766 \n",
      "Epoch:28 Learning rate 0.00179738978880607 Weight Decay 1e-07 Train Loss:0.8612138628959656  Valid Loss:0.9094685316085815 \n",
      "Epoch:29 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.862501323223114  Valid Loss:0.9098938703536987 \n",
      "Epoch:30 Learning rate 0.0010773910507136221 Weight Decay 1e-07 Train Loss:0.859920084476471  Valid Loss:0.9091135263442993 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,None,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[50,10],[0.25,0.25,0]).to(device)\n",
    "wd=1e-6\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=3e-2,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0,start_lr=3e-2,end_lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.03 Weight Decay 1e-06 Train Loss:1.121293306350708  Valid Loss:1.1241103410720825 \n",
      "Epoch:1 Learning rate 0.03 Weight Decay 1e-06 Train Loss:1.0636924505233765  Valid Loss:1.0565012693405151 \n",
      "Epoch:2 Learning rate 0.03 Weight Decay 1e-06 Train Loss:1.049501895904541  Valid Loss:1.0456959009170532 \n",
      "Epoch:3 Learning rate 0.03 Weight Decay 1e-06 Train Loss:1.054572582244873  Valid Loss:1.045621633529663 \n",
      "Epoch:4 Learning rate 0.03 Weight Decay 1e-06 Train Loss:1.0274609327316284  Valid Loss:1.0315592288970947 \n",
      "Epoch:5 Learning rate 0.03 Weight Decay 1e-06 Train Loss:1.012212872505188  Valid Loss:1.0096160173416138 \n",
      "Epoch:6 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9999094605445862  Valid Loss:0.9935700297355652 \n",
      "Epoch:7 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9821175336837769  Valid Loss:0.9778319001197815 \n",
      "Epoch:8 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9813492298126221  Valid Loss:0.9774125814437866 \n",
      "Epoch:9 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9697121381759644  Valid Loss:0.9719650149345398 \n",
      "Epoch:10 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9619137644767761  Valid Loss:0.9637266397476196 \n",
      "Epoch:11 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9493858218193054  Valid Loss:0.9540689587593079 \n",
      "Epoch:12 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9377466440200806  Valid Loss:0.9337668418884277 \n",
      "Epoch:13 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9263108968734741  Valid Loss:0.9319714903831482 \n",
      "Epoch:14 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9245807528495789  Valid Loss:0.9312058687210083 \n",
      "Epoch:15 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9166314005851746  Valid Loss:0.9250539541244507 \n",
      "Epoch:16 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9115710854530334  Valid Loss:0.9236840605735779 \n",
      "Epoch:17 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9074445366859436  Valid Loss:0.9219580292701721 \n",
      "Epoch:18 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9148785471916199  Valid Loss:0.9295663237571716 \n",
      "Epoch:19 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9106046557426453  Valid Loss:0.9235419034957886 \n",
      "Epoch:20 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9062322378158569  Valid Loss:0.9230314493179321 \n",
      "Epoch:21 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.9070519804954529  Valid Loss:0.9252784252166748 \n",
      "Epoch:22 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8922168612480164  Valid Loss:0.9193464517593384 \n",
      "Epoch:23 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8937787413597107  Valid Loss:0.9112020134925842 \n",
      "Epoch:24 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8912152647972107  Valid Loss:0.9224445223808289 \n",
      "Epoch:25 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8905864357948303  Valid Loss:0.9232610464096069 \n",
      "Epoch:26 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8874543905258179  Valid Loss:0.9222835898399353 \n",
      "Epoch:27 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8888351321220398  Valid Loss:0.9216768145561218 \n",
      "Epoch:28 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8855698108673096  Valid Loss:0.9215481281280518 \n",
      "Epoch:29 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8788430094718933  Valid Loss:0.9174898862838745 \n",
      "Epoch:30 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.879767894744873  Valid Loss:0.9301835894584656 \n",
      "Epoch:31 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8826940655708313  Valid Loss:0.921703040599823 \n",
      "Epoch:32 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.882546067237854  Valid Loss:0.920102059841156 \n",
      "Epoch:33 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.883468508720398  Valid Loss:0.9202139973640442 \n",
      "Epoch:34 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8841244578361511  Valid Loss:0.9260911345481873 \n",
      "Epoch:35 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8808163404464722  Valid Loss:0.9163323640823364 \n",
      "Epoch:36 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8826276659965515  Valid Loss:0.9149051308631897 \n",
      "Epoch:37 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8830028176307678  Valid Loss:0.9357573986053467 \n",
      "Epoch:38 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8780984282493591  Valid Loss:0.9295669198036194 \n",
      "Epoch:39 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8823846578598022  Valid Loss:0.9300456047058105 \n",
      "Epoch:40 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.88735431432724  Valid Loss:0.9239208102226257 \n",
      "Epoch:41 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8827447295188904  Valid Loss:0.9141337871551514 \n",
      "Epoch:42 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8923722505569458  Valid Loss:0.9271693825721741 \n",
      "Epoch:43 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8825140595436096  Valid Loss:0.9267642498016357 \n",
      "Epoch:44 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8868836164474487  Valid Loss:0.9273173809051514 \n",
      "Epoch:45 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8772467374801636  Valid Loss:0.92718505859375 \n",
      "Epoch:46 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8816462159156799  Valid Loss:0.9207386374473572 \n",
      "Epoch:47 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8863244652748108  Valid Loss:0.9240967631340027 \n",
      "Epoch:48 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8949728608131409  Valid Loss:0.9203200340270996 \n",
      "Epoch:49 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8861664533615112  Valid Loss:0.9303192496299744 \n",
      "Epoch:50 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8809376955032349  Valid Loss:0.9260409474372864 \n",
      "Epoch:51 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8846452832221985  Valid Loss:0.9280954003334045 \n",
      "Epoch:52 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8793050050735474  Valid Loss:0.9178721904754639 \n",
      "Epoch:53 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8804308176040649  Valid Loss:0.9296370148658752 \n",
      "Epoch:54 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8799658417701721  Valid Loss:0.9271307587623596 \n",
      "Epoch:55 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8810829520225525  Valid Loss:0.9207638502120972 \n",
      "Epoch:56 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8786779642105103  Valid Loss:0.9215579032897949 \n",
      "Epoch:57 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8820477724075317  Valid Loss:0.9329763650894165 \n",
      "Epoch:58 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8831784129142761  Valid Loss:0.9159582257270813 \n",
      "Epoch:59 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.8743923902511597  Valid Loss:0.9175237417221069 \n",
      "Epoch:60 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.881841778755188  Valid Loss:0.9262624382972717 \n",
      "Epoch:61 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.88524329662323  Valid Loss:0.9181553721427917 \n",
      "Epoch:62 Learning rate 0.03 Weight Decay 1e-06 Train Loss:0.884468674659729  Valid Loss:0.9203165769577026 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,None,63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mtx_1_weights=autoenc.encoder[0][0].weight.data.cpu().numpy()\n",
    "item_mtx_2_weights=autoenc.encoder[1][0].weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 610), (10, 50))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_mtx_1_weights.shape, item_mtx_2_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9724, 610)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.997642</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.998100</td>\n",
       "      <td>0.961256</td>\n",
       "      <td>0.172133</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>0.999146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.698399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.997402</td>\n",
       "      <td>0.968649</td>\n",
       "      <td>0.969533</td>\n",
       "      <td>-0.995085</td>\n",
       "      <td>-0.879193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.918195</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.579384</td>\n",
       "      <td>-0.997684</td>\n",
       "      <td>0.973646</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.998660</td>\n",
       "      <td>-0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.049580</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.326456</td>\n",
       "      <td>-0.994506</td>\n",
       "      <td>0.900776</td>\n",
       "      <td>0.998242</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>-0.999805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.706495</td>\n",
       "      <td>-0.999985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.996118</td>\n",
       "      <td>-0.992763</td>\n",
       "      <td>0.954681</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.551115</td>\n",
       "      <td>-0.999920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.998133</td>\n",
       "      <td>0.953757</td>\n",
       "      <td>0.701939</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>0.985551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.604212</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>-0.993037</td>\n",
       "      <td>0.946007</td>\n",
       "      <td>0.999249</td>\n",
       "      <td>0.845868</td>\n",
       "      <td>-0.999964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.659138</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.825122</td>\n",
       "      <td>-0.998228</td>\n",
       "      <td>0.955654</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.998201</td>\n",
       "      <td>-0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946655</td>\n",
       "      <td>-0.995916</td>\n",
       "      <td>0.965868</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999967</td>\n",
       "      <td>-0.129109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.997515</td>\n",
       "      <td>0.972197</td>\n",
       "      <td>0.990539</td>\n",
       "      <td>-0.987088</td>\n",
       "      <td>-0.879916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.988547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.995225</td>\n",
       "      <td>0.946944</td>\n",
       "      <td>0.794103</td>\n",
       "      <td>-0.976130</td>\n",
       "      <td>-0.834880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999017</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.995468</td>\n",
       "      <td>0.970343</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.964582</td>\n",
       "      <td>-0.999976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.987809</td>\n",
       "      <td>-0.998061</td>\n",
       "      <td>0.930190</td>\n",
       "      <td>0.999836</td>\n",
       "      <td>-0.461400</td>\n",
       "      <td>-0.994339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999917</td>\n",
       "      <td>-0.998920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999976</td>\n",
       "      <td>-0.996188</td>\n",
       "      <td>0.949489</td>\n",
       "      <td>0.889557</td>\n",
       "      <td>-0.513722</td>\n",
       "      <td>-0.993430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.647145</td>\n",
       "      <td>-0.999796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.977091</td>\n",
       "      <td>-0.992942</td>\n",
       "      <td>0.911925</td>\n",
       "      <td>0.989372</td>\n",
       "      <td>0.582060</td>\n",
       "      <td>-0.991353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.998141</td>\n",
       "      <td>0.956535</td>\n",
       "      <td>-0.202402</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>0.999776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.855561</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.996099</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>0.078061</td>\n",
       "      <td>-0.999778</td>\n",
       "      <td>0.758751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999909</td>\n",
       "      <td>-0.334526</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.997348</td>\n",
       "      <td>0.957540</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>-0.998042</td>\n",
       "      <td>-0.590416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.455413</td>\n",
       "      <td>-0.999896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.990279</td>\n",
       "      <td>-0.998513</td>\n",
       "      <td>0.960123</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.426766</td>\n",
       "      <td>-0.999239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.980947</td>\n",
       "      <td>-0.380213</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>-0.999778</td>\n",
       "      <td>-0.996339</td>\n",
       "      <td>0.928683</td>\n",
       "      <td>0.993261</td>\n",
       "      <td>-0.993046</td>\n",
       "      <td>-0.118106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.997056</td>\n",
       "      <td>0.957947</td>\n",
       "      <td>-0.262358</td>\n",
       "      <td>-0.999101</td>\n",
       "      <td>0.047967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999856</td>\n",
       "      <td>-0.962590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.995779</td>\n",
       "      <td>0.954595</td>\n",
       "      <td>0.928235</td>\n",
       "      <td>-0.862089</td>\n",
       "      <td>-0.847315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999934</td>\n",
       "      <td>0.523251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.996260</td>\n",
       "      <td>0.956924</td>\n",
       "      <td>0.870085</td>\n",
       "      <td>-0.987616</td>\n",
       "      <td>0.388905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.908988</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.879664</td>\n",
       "      <td>-0.993239</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>0.941289</td>\n",
       "      <td>-0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999484</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.998360</td>\n",
       "      <td>0.957723</td>\n",
       "      <td>-0.953258</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.999779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995662</td>\n",
       "      <td>-0.732458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.996310</td>\n",
       "      <td>0.958731</td>\n",
       "      <td>0.743899</td>\n",
       "      <td>-0.992434</td>\n",
       "      <td>-0.989606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.834202</td>\n",
       "      <td>-0.999915</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.693230</td>\n",
       "      <td>-0.989778</td>\n",
       "      <td>0.911298</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>0.988530</td>\n",
       "      <td>-0.999871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.783882</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.988239</td>\n",
       "      <td>0.901348</td>\n",
       "      <td>0.694719</td>\n",
       "      <td>-0.999354</td>\n",
       "      <td>0.491486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.999845</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.997267</td>\n",
       "      <td>0.932634</td>\n",
       "      <td>-0.994803</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>0.940763</td>\n",
       "      <td>0.999860</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.998990</td>\n",
       "      <td>0.886273</td>\n",
       "      <td>-0.168093</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>0.850365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188189</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999432</td>\n",
       "      <td>0.038402</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>-0.997068</td>\n",
       "      <td>-0.999043</td>\n",
       "      <td>0.868174</td>\n",
       "      <td>0.846120</td>\n",
       "      <td>-0.968985</td>\n",
       "      <td>0.849030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188301</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.959457</td>\n",
       "      <td>-0.980568</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.993325</td>\n",
       "      <td>-0.999342</td>\n",
       "      <td>0.920576</td>\n",
       "      <td>0.992880</td>\n",
       "      <td>-0.699138</td>\n",
       "      <td>-0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188675</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>0.951890</td>\n",
       "      <td>0.997579</td>\n",
       "      <td>-0.999922</td>\n",
       "      <td>-0.998597</td>\n",
       "      <td>0.873518</td>\n",
       "      <td>0.518849</td>\n",
       "      <td>-0.999095</td>\n",
       "      <td>0.999965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188751</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>0.994580</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.999140</td>\n",
       "      <td>0.857987</td>\n",
       "      <td>-0.732043</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.999964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188797</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>0.943863</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.998030</td>\n",
       "      <td>0.881419</td>\n",
       "      <td>-0.080462</td>\n",
       "      <td>-0.999840</td>\n",
       "      <td>0.996316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188833</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.972904</td>\n",
       "      <td>0.995009</td>\n",
       "      <td>-0.999956</td>\n",
       "      <td>-0.998585</td>\n",
       "      <td>0.877108</td>\n",
       "      <td>0.497268</td>\n",
       "      <td>-0.999516</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189043</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>0.941089</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>-0.999786</td>\n",
       "      <td>-0.999392</td>\n",
       "      <td>0.873818</td>\n",
       "      <td>0.430197</td>\n",
       "      <td>-0.997987</td>\n",
       "      <td>0.993704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189111</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>0.965977</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>-0.999859</td>\n",
       "      <td>-0.999432</td>\n",
       "      <td>0.874501</td>\n",
       "      <td>0.317692</td>\n",
       "      <td>-0.998702</td>\n",
       "      <td>0.996188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189333</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.994664</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999614</td>\n",
       "      <td>0.851951</td>\n",
       "      <td>-0.671621</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>0.999956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189381</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>0.896576</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>-0.999826</td>\n",
       "      <td>-0.998651</td>\n",
       "      <td>0.870249</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>-0.997963</td>\n",
       "      <td>0.999805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189547</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999560</td>\n",
       "      <td>0.100116</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>-0.998087</td>\n",
       "      <td>-0.999081</td>\n",
       "      <td>0.881613</td>\n",
       "      <td>0.897090</td>\n",
       "      <td>-0.961333</td>\n",
       "      <td>0.876824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189713</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.735710</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.999289</td>\n",
       "      <td>0.819479</td>\n",
       "      <td>-0.999170</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190183</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.998369</td>\n",
       "      <td>0.999260</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999610</td>\n",
       "      <td>0.915978</td>\n",
       "      <td>-0.180568</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>0.999870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190207</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999940</td>\n",
       "      <td>0.805997</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>-0.999442</td>\n",
       "      <td>-0.999284</td>\n",
       "      <td>0.871954</td>\n",
       "      <td>0.635986</td>\n",
       "      <td>-0.994505</td>\n",
       "      <td>0.979750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190209</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>0.987154</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>-0.999931</td>\n",
       "      <td>-0.999491</td>\n",
       "      <td>0.875402</td>\n",
       "      <td>0.095197</td>\n",
       "      <td>-0.999390</td>\n",
       "      <td>0.998356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190213</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999877</td>\n",
       "      <td>0.646165</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.999051</td>\n",
       "      <td>-0.999215</td>\n",
       "      <td>0.870793</td>\n",
       "      <td>0.721090</td>\n",
       "      <td>-0.990411</td>\n",
       "      <td>0.961093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190215</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999940</td>\n",
       "      <td>0.805997</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>-0.999442</td>\n",
       "      <td>-0.999284</td>\n",
       "      <td>0.871954</td>\n",
       "      <td>0.635986</td>\n",
       "      <td>-0.994505</td>\n",
       "      <td>0.979750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190219</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999877</td>\n",
       "      <td>0.646165</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>-0.999051</td>\n",
       "      <td>-0.999215</td>\n",
       "      <td>0.870793</td>\n",
       "      <td>0.721090</td>\n",
       "      <td>-0.990411</td>\n",
       "      <td>0.961093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190221</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999432</td>\n",
       "      <td>0.038402</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>-0.997068</td>\n",
       "      <td>-0.999043</td>\n",
       "      <td>0.868174</td>\n",
       "      <td>0.846120</td>\n",
       "      <td>-0.968985</td>\n",
       "      <td>0.849030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191005</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.998932</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.999662</td>\n",
       "      <td>0.854483</td>\n",
       "      <td>-0.576428</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193565</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.999626</td>\n",
       "      <td>0.857625</td>\n",
       "      <td>-0.376126</td>\n",
       "      <td>-0.999961</td>\n",
       "      <td>0.999283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193567</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>0.999871</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.999600</td>\n",
       "      <td>0.859338</td>\n",
       "      <td>-0.231004</td>\n",
       "      <td>-0.999921</td>\n",
       "      <td>0.998789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193571</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999176</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.999646</td>\n",
       "      <td>0.856007</td>\n",
       "      <td>-0.489336</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>0.999548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193573</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999176</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.999646</td>\n",
       "      <td>0.856007</td>\n",
       "      <td>-0.489336</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>0.999548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193579</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.999626</td>\n",
       "      <td>0.857625</td>\n",
       "      <td>-0.376126</td>\n",
       "      <td>-0.999961</td>\n",
       "      <td>0.999283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193581</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.999176</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.999646</td>\n",
       "      <td>0.856007</td>\n",
       "      <td>-0.489336</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>0.999548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193583</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.999626</td>\n",
       "      <td>0.857625</td>\n",
       "      <td>-0.376126</td>\n",
       "      <td>-0.999961</td>\n",
       "      <td>0.999283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193585</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.999626</td>\n",
       "      <td>0.857625</td>\n",
       "      <td>-0.376126</td>\n",
       "      <td>-0.999961</td>\n",
       "      <td>0.999283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193587</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-0.999626</td>\n",
       "      <td>0.857625</td>\n",
       "      <td>-0.376126</td>\n",
       "      <td>-0.999961</td>\n",
       "      <td>0.999283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193609</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.967086</td>\n",
       "      <td>0.998740</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.999763</td>\n",
       "      <td>0.898250</td>\n",
       "      <td>0.908199</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>0.996192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "movieId                                                                         \n",
       "1       -1.000000 -1.000000  0.997642  0.999984 -1.000000 -0.998100  0.961256   \n",
       "2       -1.000000 -0.999997 -0.698399  1.000000 -1.000000 -0.997402  0.968649   \n",
       "3       -1.000000  0.918195 -1.000000  1.000000 -0.579384 -0.997684  0.973646   \n",
       "4       -1.000000  0.049580 -0.999999  1.000000 -0.326456 -0.994506  0.900776   \n",
       "5       -1.000000 -0.706495 -0.999985  1.000000 -0.996118 -0.992763  0.954681   \n",
       "6       -1.000000 -1.000000  0.997260  0.999933 -1.000000 -0.998133  0.953757   \n",
       "7       -1.000000 -0.604212 -0.999999  1.000000 -0.983496 -0.993037  0.946007   \n",
       "8       -1.000000  0.659138 -1.000000  1.000000 -0.825122 -0.998228  0.955654   \n",
       "9       -1.000000  0.999210 -1.000000  1.000000  0.946655 -0.995916  0.965868   \n",
       "10      -1.000000 -0.999967 -0.129109  1.000000 -0.999998 -0.997515  0.972197   \n",
       "11      -1.000000 -0.999997 -0.988547  1.000000 -0.999999 -0.995225  0.946944   \n",
       "12      -1.000000  0.999017 -1.000000  1.000000  0.809631 -0.995468  0.970343   \n",
       "13      -1.000000 -0.964582 -0.999976  1.000000 -0.987809 -0.998061  0.930190   \n",
       "14      -1.000000 -0.999917 -0.998920  1.000000 -0.999976 -0.996188  0.949489   \n",
       "15      -1.000000 -0.647145 -0.999796  1.000000 -0.977091 -0.992942  0.911925   \n",
       "16      -1.000000 -1.000000  0.999111  0.999940 -1.000000 -0.998141  0.956535   \n",
       "17      -1.000000 -1.000000  0.855561  0.999999 -1.000000 -0.996099  0.951428   \n",
       "18      -1.000000 -0.999909 -0.334526  0.999999 -0.999997 -0.997348  0.957540   \n",
       "19      -1.000000 -0.455413 -0.999896  1.000000 -0.990279 -0.998513  0.960123   \n",
       "20      -1.000000 -0.980947 -0.380213  0.999996 -0.999778 -0.996339  0.928683   \n",
       "21      -1.000000 -1.000000  0.020448  1.000000 -1.000000 -0.997056  0.957947   \n",
       "22      -1.000000 -0.999856 -0.962590  1.000000 -0.999977 -0.995779  0.954595   \n",
       "23      -1.000000 -0.999934  0.523251  1.000000 -0.999997 -0.996260  0.956924   \n",
       "24      -1.000000  0.908988 -0.999999  1.000000 -0.879664 -0.993239  0.958880   \n",
       "25      -1.000000 -1.000000  0.999484  0.999983 -1.000000 -0.998360  0.957723   \n",
       "26      -1.000000 -0.995662 -0.732458  1.000000 -0.999993 -0.996310  0.958731   \n",
       "27      -1.000000  0.834202 -0.999915  1.000000 -0.693230 -0.989778  0.911298   \n",
       "28      -0.999999 -0.999999 -0.783882  0.999974 -0.999998 -0.988239  0.901348   \n",
       "29      -1.000000 -1.000000  0.999986  0.999845 -1.000000 -0.997267  0.932634   \n",
       "30      -1.000000 -0.999996  0.940763  0.999860 -1.000000 -0.998990  0.886273   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "188189  -1.000000 -0.999432  0.038402  0.999978 -0.997068 -0.999043  0.868174   \n",
       "188301  -1.000000 -0.959457 -0.980568  1.000000 -0.993325 -0.999342  0.920576   \n",
       "188675  -0.999999 -0.999998  0.951890  0.997579 -0.999922 -0.998597  0.873518   \n",
       "188751  -1.000000 -1.000000  0.999814  0.994580 -0.999999 -0.999140  0.857987   \n",
       "188797  -1.000000 -0.999991  0.943863  0.999607 -0.999987 -0.998030  0.881419   \n",
       "188833  -0.999999 -0.999999  0.972904  0.995009 -0.999956 -0.998585  0.877108   \n",
       "189043  -1.000000 -0.999983  0.941089  0.999918 -0.999786 -0.999392  0.873818   \n",
       "189111  -1.000000 -0.999990  0.965977  0.999898 -0.999859 -0.999432  0.874501   \n",
       "189333  -1.000000 -1.000000  0.999998  0.994664 -1.000000 -0.999614  0.851951   \n",
       "189381  -1.000000 -0.999995  0.896576  0.999099 -0.999826 -0.998651  0.870249   \n",
       "189547  -1.000000 -0.999560  0.100116  0.999982 -0.998087 -0.999081  0.881613   \n",
       "189713  -0.999998 -1.000000  0.999916  0.735710 -0.999998 -0.999289  0.819479   \n",
       "190183  -1.000000 -1.000000  0.998369  0.999260 -1.000000 -0.999610  0.915978   \n",
       "190207  -1.000000 -0.999940  0.805997  0.999950 -0.999442 -0.999284  0.871954   \n",
       "190209  -1.000000 -0.999996  0.987154  0.999849 -0.999931 -0.999491  0.875402   \n",
       "190213  -1.000000 -0.999877  0.646165  0.999962 -0.999051 -0.999215  0.870793   \n",
       "190215  -1.000000 -0.999940  0.805997  0.999950 -0.999442 -0.999284  0.871954   \n",
       "190219  -1.000000 -0.999877  0.646165  0.999962 -0.999051 -0.999215  0.870793   \n",
       "190221  -1.000000 -0.999432  0.038402  0.999978 -0.997068 -0.999043  0.868174   \n",
       "191005  -1.000000 -0.999998  0.999993  0.998932 -0.999999 -0.999662  0.854483   \n",
       "193565  -1.000000 -0.999997  0.999956  0.999384 -0.999997 -0.999626  0.857625   \n",
       "193567  -1.000000 -0.999995  0.999871  0.999556 -0.999995 -0.999600  0.859338   \n",
       "193571  -1.000000 -0.999998  0.999983  0.999176 -0.999998 -0.999646  0.856007   \n",
       "193573  -1.000000 -0.999998  0.999983  0.999176 -0.999998 -0.999646  0.856007   \n",
       "193579  -1.000000 -0.999997  0.999956  0.999384 -0.999997 -0.999626  0.857625   \n",
       "193581  -1.000000 -0.999998  0.999983  0.999176 -0.999998 -0.999646  0.856007   \n",
       "193583  -1.000000 -0.999997  0.999956  0.999384 -0.999997 -0.999626  0.857625   \n",
       "193585  -1.000000 -0.999997  0.999956  0.999384 -0.999997 -0.999626  0.857625   \n",
       "193587  -1.000000 -0.999997  0.999956  0.999384 -0.999997 -0.999626  0.857625   \n",
       "193609  -1.000000 -1.000000  0.967086  0.998740 -0.999998 -0.999763  0.898250   \n",
       "\n",
       "                7         8         9  \n",
       "movieId                                \n",
       "1        0.172133 -0.999995  0.999146  \n",
       "2        0.969533 -0.995085 -0.879193  \n",
       "3        0.999997  0.998660 -0.999999  \n",
       "4        0.998242  0.998951 -0.999805  \n",
       "5        0.999275  0.551115 -0.999920  \n",
       "6        0.701939 -0.999873  0.985551  \n",
       "7        0.999249  0.845868 -0.999964  \n",
       "8        0.999956  0.998201 -0.999997  \n",
       "9        0.999993  0.999756 -1.000000  \n",
       "10       0.990539 -0.987088 -0.879916  \n",
       "11       0.794103 -0.976130 -0.834880  \n",
       "12       0.999999  0.999798 -1.000000  \n",
       "13       0.999836 -0.461400 -0.994339  \n",
       "14       0.889557 -0.513722 -0.993430  \n",
       "15       0.989372  0.582060 -0.991353  \n",
       "16      -0.202402 -0.999996  0.999776  \n",
       "17       0.078061 -0.999778  0.758751  \n",
       "18       0.994425 -0.998042 -0.590416  \n",
       "19       0.999706  0.426766 -0.999239  \n",
       "20       0.993261 -0.993046 -0.118106  \n",
       "21      -0.262358 -0.999101  0.047967  \n",
       "22       0.928235 -0.862089 -0.847315  \n",
       "23       0.870085 -0.987616  0.388905  \n",
       "24       0.999857  0.941289 -0.999995  \n",
       "25      -0.953258 -0.999997  0.999779  \n",
       "26       0.743899 -0.992434 -0.989606  \n",
       "27       0.999002  0.988530 -0.999871  \n",
       "28       0.694719 -0.999354  0.491486  \n",
       "29      -0.994803 -1.000000  0.999977  \n",
       "30      -0.168093 -0.999993  0.850365  \n",
       "...           ...       ...       ...  \n",
       "188189   0.846120 -0.968985  0.849030  \n",
       "188301   0.992880 -0.699138 -0.001511  \n",
       "188675   0.518849 -0.999095  0.999965  \n",
       "188751  -0.732043 -0.999999  0.999964  \n",
       "188797  -0.080462 -0.999840  0.996316  \n",
       "188833   0.497268 -0.999516  0.999990  \n",
       "189043   0.430197 -0.997987  0.993704  \n",
       "189111   0.317692 -0.998702  0.996188  \n",
       "189333  -0.671621 -0.999998  0.999956  \n",
       "189381   0.584816 -0.997963  0.999805  \n",
       "189547   0.897090 -0.961333  0.876824  \n",
       "189713  -0.999170 -0.999997  0.999999  \n",
       "190183  -0.180568 -0.999995  0.999870  \n",
       "190207   0.635986 -0.994505  0.979750  \n",
       "190209   0.095197 -0.999390  0.998356  \n",
       "190213   0.721090 -0.990411  0.961093  \n",
       "190215   0.635986 -0.994505  0.979750  \n",
       "190219   0.721090 -0.990411  0.961093  \n",
       "190221   0.846120 -0.968985  0.849030  \n",
       "191005  -0.576428 -0.999989  0.999700  \n",
       "193565  -0.376126 -0.999961  0.999283  \n",
       "193567  -0.231004 -0.999921  0.998789  \n",
       "193571  -0.489336 -0.999980  0.999548  \n",
       "193573  -0.489336 -0.999980  0.999548  \n",
       "193579  -0.376126 -0.999961  0.999283  \n",
       "193581  -0.489336 -0.999980  0.999548  \n",
       "193583  -0.376126 -0.999961  0.999283  \n",
       "193585  -0.376126 -0.999961  0.999283  \n",
       "193587  -0.376126 -0.999961  0.999283  \n",
       "193609   0.908199 -0.999954  0.996192  \n",
       "\n",
       "[9724 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(expit(df_train@item_mtx_1_weights.T)@item_mtx_2_weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.1851178407669067  Valid Loss:1.0310707092285156 \n",
      "Epoch:1 Learning rate 0.0038729833462074164 Weight Decay 1e-07 Train Loss:1.003689169883728  Valid Loss:0.9441735148429871 \n",
      "Epoch:2 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:0.9867559671401978  Valid Loss:0.9520727396011353 \n",
      "Epoch:3 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.0897201299667358  Valid Loss:0.9591107964515686 \n",
      "Epoch:4 Learning rate 0.010779123358892525 Weight Decay 1e-07 Train Loss:0.9829736351966858  Valid Loss:0.9768510460853577 \n",
      "Epoch:5 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:0.948708176612854  Valid Loss:0.9342473745346069 \n",
      "Epoch:6 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.9373030662536621  Valid Loss:0.9225978851318359 \n",
      "Epoch:7 Learning rate 0.0004999999999999998 Weight Decay 1e-07 Train Loss:0.9346903562545776  Valid Loss:0.9241189360618591 \n",
      "Epoch:8 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.0187814235687256  Valid Loss:0.9331497550010681 \n",
      "Epoch:9 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:0.9641764760017395  Valid Loss:0.967984139919281 \n",
      "Epoch:10 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:0.936285138130188  Valid Loss:0.9378364086151123 \n",
      "Epoch:11 Learning rate 0.006461220105808663 Weight Decay 1e-07 Train Loss:0.9152931571006775  Valid Loss:0.9325439929962158 \n",
      "Epoch:12 Learning rate 0.0038729833462074173 Weight Decay 1e-07 Train Loss:0.9061939716339111  Valid Loss:0.9123200178146362 \n",
      "Epoch:13 Learning rate 0.0023215429523156072 Weight Decay 1e-07 Train Loss:0.8979648947715759  Valid Loss:0.9068906903266907 \n",
      "Epoch:14 Learning rate 0.0013915788418568708 Weight Decay 1e-07 Train Loss:0.8955093026161194  Valid Loss:0.8947471976280212 \n",
      "Epoch:15 Learning rate 0.0008341399288659162 Weight Decay 1e-07 Train Loss:0.8892868757247925  Valid Loss:0.8957937955856323 \n",
      "Epoch:16 Learning rate 0.0005000000000000002 Weight Decay 1e-07 Train Loss:0.8887911438941956  Valid Loss:0.8957212567329407 \n",
      "Epoch:17 Learning rate 0.03 Weight Decay 1e-07 Train Loss:0.9504203796386719  Valid Loss:0.9304328560829163 \n",
      "Epoch:18 Learning rate 0.02322666208281794 Weight Decay 1e-07 Train Loss:0.9274903535842896  Valid Loss:0.9204272031784058 \n",
      "Epoch:19 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:0.9134508371353149  Valid Loss:0.8887390494346619 \n",
      "Epoch:20 Learning rate 0.013922521437378356 Weight Decay 1e-07 Train Loss:0.8995663523674011  Valid Loss:0.9021214246749878 \n",
      "Epoch:21 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:0.8879196643829346  Valid Loss:0.8885521292686462 \n",
      "Epoch:22 Learning rate 0.00834543519353354 Weight Decay 1e-07 Train Loss:0.879929780960083  Valid Loss:0.8917460441589355 \n",
      "Epoch:23 Learning rate 0.006461220105808662 Weight Decay 1e-07 Train Loss:0.879179835319519  Valid Loss:0.8828877210617065 \n",
      "Epoch:24 Learning rate 0.005002419201344232 Weight Decay 1e-07 Train Loss:0.8680992126464844  Valid Loss:0.8840966820716858 \n",
      "Epoch:25 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:0.866870105266571  Valid Loss:0.8734056353569031 \n",
      "Epoch:26 Learning rate 0.002998549181158038 Weight Decay 1e-07 Train Loss:0.8624358773231506  Valid Loss:0.8727845549583435 \n",
      "Epoch:27 Learning rate 0.002321542952315606 Weight Decay 1e-07 Train Loss:0.8567821383476257  Valid Loss:0.8782932162284851 \n",
      "Epoch:28 Learning rate 0.00179738978880607 Weight Decay 1e-07 Train Loss:0.856454074382782  Valid Loss:0.8703406453132629 \n",
      "Epoch:29 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.855315089225769  Valid Loss:0.8694369792938232 \n",
      "Epoch:30 Learning rate 0.0010773910507136221 Weight Decay 1e-07 Train Loss:0.8493168354034424  Valid Loss:0.8707003593444824 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 610])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[0][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[1][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mtx_1_weights=autoenc.encoder[0][0].weight.data.cpu().numpy()\n",
    "item_mtx_2_weights=autoenc.encoder[1][0].weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([item_mtx_1_weights,item_mtx_2_weights],open(f'{DATAPATH}/inter/item_autoenc_weights.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
