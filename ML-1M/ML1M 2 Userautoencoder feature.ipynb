{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/home/kirana/Documents/phd/exp3_autoencoder'\n",
    "DATAPATH='/home/kirana/Documents/final_dissertation_final/experiments/datasets/ml-1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df, df_train,df_valid,df,df_ratings,idx_to_user,\\\n",
    "             idx_to_movie,movie_to_idx,user_to_idx]=pickle.load(open(f'{DATAPATH}/reads.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6041, 3707), (6041, 3707))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape,df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3707 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 95 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3707)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[1].todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3707,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[1].todense().getA1().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencdata (Dataset):\n",
    "    def __init__(self,dfX,dfXv):\n",
    "        self.dfX,self.dfXv=dfX,dfXv\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.dfX.shape[0]\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        return torch.FloatTensor(self.dfX[idx].todense().getA1()),torch.FloatTensor(self.dfXv[idx].todense().getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=autoencdata(df_train, df_valid)\n",
    "#dsvalid=autoencdata(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader object\n",
    "dltrain=DataLoader(dstrain,batch_size=bs,shuffle=True)\n",
    "#dlvalid=DataLoader(dsvalid,batch_size=bs,shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.min(),df_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6041, 3707)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6041, 3707)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6041 users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model Architecture for the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(x,y,dropout,activation=nn.Sigmoid()):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(x, y),\n",
    "        activation,\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.BatchNorm1d(y)\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder (nn.Module):    \n",
    "    def __init__(self,n_inp=9724,hidden=[50,10],dropouts=[0,0,0],rating_range=[0.5,5]):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.hidden,self.dropouts,self.rating_range=n_inp,hidden,dropouts,rating_range\n",
    "        encoder=[hidden_layer(n_inp if i==0 else hidden[i-1],hidden[i],dropouts[i],\\\n",
    "                              nn.Sigmoid() if i<len(hidden)-1 else nn.Sigmoid()) for i in range(len(hidden))]\n",
    "        self.encoder=nn.Sequential(*encoder)\n",
    "        hidden=hidden[::-1]\n",
    "        num_steps=len(hidden)-1\n",
    "        dropouts=dropouts[num_steps:]\n",
    "        decoder=[hidden_layer(hidden[i],hidden[i+1] if i<len(hidden)-1 else n_inp,dropouts[i]) for i in range(len(hidden)-1)]\n",
    "        self.decoder=nn.Sequential(*decoder)\n",
    "        self.fc=nn.Linear(hidden[-1],n_inp)\n",
    "        self.initialize()\n",
    "        self.criterion=nn.MSELoss()\n",
    "    \n",
    "    def initialize(self):\n",
    "        for x in self.encoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "        for x in self.decoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "\n",
    "    def forward (self,Xb):\n",
    "        encoded=self.encoder(Xb)\n",
    "        decoded=self.decoder(encoded)\n",
    "        out=self.fc(decoded)\n",
    "        outv=out.clone()\n",
    "        out[Xb==0]=0\n",
    "        loss=self.criterion(out,Xb)\n",
    "        return outv,loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_final_user=10 # 3 for IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[25,n_final_user],[0.2,0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=3707, out_features=25, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.2)\n",
       "      (3): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=25, out_features=10, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.2)\n",
       "      (3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=25, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.2)\n",
       "      (3): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=25, out_features=3707, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 5.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 4.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for Xb,Xb_v in dltrain:\n",
    "    print (Xb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3707])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 5.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 4.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,loss,_=autoenc.forward(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3707])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.MSELoss"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-7\n",
    "#wd=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "#optimizer=torch.optim.SGD(model_sentiment.parameters(),lr=1e-2,momentum=0.9, weight_decay=wd)\n",
    "metric_fn=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dltrain.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None,\\\n",
    "                 cycle_mult=0,lr_decay=0.7,wd_mult=6,start_lr=2e-2, end_lr=5e-4):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "        self.cycle_mult,self.lr_decay=cycle_mult,lr_decay\n",
    "        self.wd_mult=wd_mult\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            self.start_lr=param_group['lr']\n",
    "            self.start_wd=param_group['weight_decay']\n",
    "        self.wd=self.start_wd\n",
    "        self.lr=self.start_lr\n",
    "        self.end_lr=end_lr\n",
    "        self.n_epoch=0\n",
    "        self.lrs=[1e-2,5e-3,1e-4,5e-4]\n",
    "        self.preds,self.preds_valid,self.trainY,self.actual=[],[],[],[]\n",
    "        self.ratio=self.end_lr/self.start_lr\n",
    "        self.num_steps=self.cycle_mult\n",
    "        self.reset_cycle=self.cycle_mult\n",
    "        \n",
    "    def fit (self,Xb,Xb_v,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        \n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        preds,loss,preds_train=self.model(Xb)\n",
    "        # denominator is the average of the error with non-zero ratings\n",
    "\n",
    "        mean_corrector = Xb.size(0)*Xb.size(1)/(torch.sum(Xb > 0).float() + 1e-10)\n",
    "        mean_corrector_v = Xb_v.size(0)*Xb_v.size(1)/(torch.sum(Xb_v > 0).float() + 1e-10)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds[Xb_v==0]=0\n",
    "            loss_v=self.model.criterion(preds,Xb_v)\n",
    "            \n",
    "            if self.metric_fn is not None:\n",
    "                acc=self.metric_fn(preds,Yb.view(-1),self.device)\n",
    "                acc=acc.item()\n",
    "\n",
    "                if 1==0:\n",
    "                    if mode_train:\n",
    "                        self.trainY.append(Yb.view(-1))\n",
    "                        self.preds.append(preds.data)\n",
    "                    else:\n",
    "                        self.actual.append(Yb.view(-1))\n",
    "                        self.preds_valid.append(preds.data)\n",
    "            else:\n",
    "                acc=0\n",
    "                acc_v=0\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            if 1==0:\n",
    "                lr =self.lrs[torch.randint(0,4,(1,))]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=torch.sqrt(loss.item()*mean_corrector)\n",
    "        myloss_v=torch.sqrt(loss_v.item()*mean_corrector_v)\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)\n",
    "        \n",
    "        return myloss, acc,myloss_v,acc_v\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        epoch_loss_v,epoch_acc_v=0,0\n",
    "\n",
    "        for Xb,Xb_v in iterator:\n",
    "            Xb=Xb.to(self.device)\n",
    "            Xb_v=Xb_v.to(self.device)\n",
    "            #Xb=Xb.squeeze(0)\n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc,loss_v,acc_v=self.fit(Xb,Xb_v,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            epoch_loss_v+=loss_v\n",
    "            epoch_acc_v+=acc_v\n",
    "            \n",
    "            k=k+1\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)} {epoch_loss_v/(k)} ')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/len(iterator)\n",
    "        epoch_acc=epoch_acc/len(iterator)\n",
    "        epoch_loss_v=epoch_loss_v/len(iterator)\n",
    "        epoch_acc_v=epoch_acc_v/len(iterator)\n",
    "            \n",
    "        return epoch_loss,epoch_acc,epoch_loss_v,epoch_acc_v\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1,ylim=None,xlim=None):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "        if xlim is not None:\n",
    "            plt.xlim(xlim)\n",
    "\n",
    "     \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        for epoch in range(n_epochs):                \n",
    "\n",
    "            loss,acc,lossv,accv=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Learning rate {self.lr} Weight Decay {self.wd} Train Loss:{loss}  Valid Loss:{lossv} ')\n",
    "  \n",
    "            if self.cycle_mult:\n",
    "                if self.n_epoch==self.reset_cycle:\n",
    "                    self.lr=self.start_lr\n",
    "                    #self.wd=self.start_wd\n",
    "                    self.reset_cycle*=self.cycle_mult\n",
    "                    #reset_cycle=self.n_epoch+reset_cycle\n",
    "                    self.n_epoch=0\n",
    "                    self.ratio=self.end_lr/self.start_lr\n",
    "                    self.num_steps=self.reset_cycle\n",
    "                else:\n",
    "                    #self.lr*=(self.lr_decay**self.n_epoch)  \n",
    "                    #if self.n_epoch>1:\n",
    "                    #    self.wd*=self.wd_mult\n",
    "                    self.lr=self.lr*(self.end_lr/self.start_lr)**(1/self.num_steps)\n",
    "                    self.n_epoch+=1\n",
    "        \n",
    "\n",
    "                \n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr']=self.lr\n",
    "                #param_group['weight_decay']=self.wd\n",
    "          \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[n_final_user*4,n_final_user],[0.2,0.2,0.2]).to(device)\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(autoenc,optimizer,None,device,0,500,0.25,cycle_mult=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=3707, out_features=40, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.2)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=40, out_features=10, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.2)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=40, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.2)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=40, out_features=3707, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain),dltrain.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(1e-4,3e-1,dltrain,len(dltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plot_lrs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plot_lrs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "del autoenc\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_final_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoenc=autoencoder(df_train.shape[1],[n_final_user*4,n_final_user],[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[n_final_user*4,n_final_user],[0.2,0.2,0.2]).to(device)\n",
    "wd=1e-7\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=7e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0,start_lr=1e-1,end_lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.007 Weight Decay 1e-07 Train Loss:1.5444715023040771  Valid Loss:1.5543502569198608 \n",
      "Epoch:1 Learning rate 0.007 Weight Decay 1e-07 Train Loss:1.002166986465454  Valid Loss:1.0008893013000488 \n",
      "Epoch:2 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.995646059513092  Valid Loss:0.9936838746070862 \n",
      "Epoch:3 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.9930841326713562  Valid Loss:0.9919650554656982 \n",
      "Epoch:4 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.9934859871864319  Valid Loss:0.9937412738800049 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,None,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=7e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=2,start_lr=7e-3,end_lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.9950498938560486  Valid Loss:0.9957966208457947 \n",
      "Epoch:1 Learning rate 0.0018708286933869708 Weight Decay 1e-07 Train Loss:0.9738324284553528  Valid Loss:0.9736164808273315 \n",
      "Epoch:2 Learning rate 0.0005 Weight Decay 1e-07 Train Loss:0.9653900265693665  Valid Loss:0.9686646461486816 \n",
      "Epoch:3 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.9845552444458008  Valid Loss:0.9840047955513 \n",
      "Epoch:4 Learning rate 0.0036188120777001943 Weight Decay 1e-07 Train Loss:0.971570611000061  Valid Loss:0.9718353748321533 \n",
      "Epoch:5 Learning rate 0.0018708286933869708 Weight Decay 1e-07 Train Loss:0.9648833870887756  Valid Loss:0.9669659733772278 \n",
      "Epoch:6 Learning rate 0.0009671682101338347 Weight Decay 1e-07 Train Loss:0.9583854675292969  Valid Loss:0.960496723651886 \n",
      "Epoch:7 Learning rate 0.0005 Weight Decay 1e-07 Train Loss:0.955618143081665  Valid Loss:0.9593086838722229 \n",
      "Epoch:8 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.9768717885017395  Valid Loss:0.979600727558136 \n",
      "Epoch:9 Learning rate 0.005033059163560604 Weight Decay 1e-07 Train Loss:0.9716797471046448  Valid Loss:0.9741731286048889 \n",
      "Epoch:10 Learning rate 0.0036188120777001947 Weight Decay 1e-07 Train Loss:0.967258095741272  Valid Loss:0.9707670211791992 \n",
      "Epoch:11 Learning rate 0.0026019564698389645 Weight Decay 1e-07 Train Loss:0.9601240754127502  Valid Loss:0.9642851948738098 \n",
      "Epoch:12 Learning rate 0.0018708286933869713 Weight Decay 1e-07 Train Loss:0.958983838558197  Valid Loss:0.962540864944458 \n",
      "Epoch:13 Learning rate 0.0013451416426719153 Weight Decay 1e-07 Train Loss:0.9553169012069702  Valid Loss:0.959757924079895 \n",
      "Epoch:14 Learning rate 0.0009671682101338352 Weight Decay 1e-07 Train Loss:0.9521965384483337  Valid Loss:0.9578724503517151 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,None,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.0006954021175312295 Weight Decay 1e-07 Train Loss:0.9510655999183655  Valid Loss:0.9563114047050476 \n",
      "Epoch:1 Learning rate 0.0005000000000000003 Weight Decay 1e-07 Train Loss:0.9497909545898438  Valid Loss:0.9562686085700989 \n",
      "Epoch:2 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.9678131341934204  Valid Loss:0.9699851870536804 \n",
      "Epoch:3 Learning rate 0.005935605625791206 Weight Decay 1e-07 Train Loss:0.9679036140441895  Valid Loss:0.9703188538551331 \n",
      "Epoch:4 Learning rate 0.005033059163560603 Weight Decay 1e-07 Train Loss:0.967369794845581  Valid Loss:0.9692065119743347 \n",
      "Epoch:5 Learning rate 0.004267750612310043 Weight Decay 1e-07 Train Loss:0.9617438316345215  Valid Loss:0.9637300968170166 \n",
      "Epoch:6 Learning rate 0.003618812077700194 Weight Decay 1e-07 Train Loss:0.9609230756759644  Valid Loss:0.9661747813224792 \n",
      "Epoch:7 Learning rate 0.003068548761011205 Weight Decay 1e-07 Train Loss:0.9561874866485596  Valid Loss:0.9591677784919739 \n",
      "Epoch:8 Learning rate 0.0026019564698389636 Weight Decay 1e-07 Train Loss:0.9513413906097412  Valid Loss:0.956397533416748 \n",
      "Epoch:9 Learning rate 0.002206312494348569 Weight Decay 1e-07 Train Loss:0.9507523775100708  Valid Loss:0.9571990966796875 \n",
      "Epoch:10 Learning rate 0.0018708286933869706 Weight Decay 1e-07 Train Loss:0.9507262706756592  Valid Loss:0.9572409391403198 \n",
      "Epoch:11 Learning rate 0.0015863573310513307 Weight Decay 1e-07 Train Loss:0.9491783380508423  Valid Loss:0.9551645517349243 \n",
      "Epoch:12 Learning rate 0.0013451416426719146 Weight Decay 1e-07 Train Loss:0.9463028907775879  Valid Loss:0.951578676700592 \n",
      "Epoch:13 Learning rate 0.0011406043288184917 Weight Decay 1e-07 Train Loss:0.9448341131210327  Valid Loss:0.9520015716552734 \n",
      "Epoch:14 Learning rate 0.0009671682101338346 Weight Decay 1e-07 Train Loss:0.9450798034667969  Valid Loss:0.9524363875389099 \n",
      "Epoch:15 Learning rate 0.0008201041527366858 Weight Decay 1e-07 Train Loss:0.9416588544845581  Valid Loss:0.9496319890022278 \n",
      "Epoch:16 Learning rate 0.000695402117531229 Weight Decay 1e-07 Train Loss:0.9424443244934082  Valid Loss:0.950609028339386 \n",
      "Epoch:17 Learning rate 0.0005896618172864973 Weight Decay 1e-07 Train Loss:0.9377139806747437  Valid Loss:0.946109414100647 \n",
      "Epoch:18 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:0.9374174475669861  Valid Loss:0.9464771151542664 \n",
      "Epoch:19 Learning rate 0.007 Weight Decay 1e-07 Train Loss:0.9647363424301147  Valid Loss:0.9683544635772705 \n",
      "Epoch:20 Learning rate 0.006445869947535278 Weight Decay 1e-07 Train Loss:0.9662199020385742  Valid Loss:0.970503032207489 \n",
      "Epoch:21 Learning rate 0.005935605625791206 Weight Decay 1e-07 Train Loss:0.9582632184028625  Valid Loss:0.9638202786445618 \n",
      "Epoch:22 Learning rate 0.005465734560529837 Weight Decay 1e-07 Train Loss:0.9575657844543457  Valid Loss:0.9619963765144348 \n",
      "Epoch:23 Learning rate 0.005033059163560602 Weight Decay 1e-07 Train Loss:0.9550501704216003  Valid Loss:0.959784984588623 \n",
      "Epoch:24 Learning rate 0.0046346349723660465 Weight Decay 1e-07 Train Loss:0.9540025591850281  Valid Loss:0.9603061079978943 \n",
      "Epoch:25 Learning rate 0.004267750612310042 Weight Decay 1e-07 Train Loss:0.9519986510276794  Valid Loss:0.9572826027870178 \n",
      "Epoch:26 Learning rate 0.003929909345066368 Weight Decay 1e-07 Train Loss:0.9487646818161011  Valid Loss:0.9551556706428528 \n",
      "Epoch:27 Learning rate 0.0036188120777001925 Weight Decay 1e-07 Train Loss:0.9489915370941162  Valid Loss:0.9552575945854187 \n",
      "Epoch:28 Learning rate 0.003332341716775053 Weight Decay 1e-07 Train Loss:0.9466882944107056  Valid Loss:0.9546893835067749 \n",
      "Epoch:29 Learning rate 0.0030685487610112036 Weight Decay 1e-07 Train Loss:0.9443261623382568  Valid Loss:0.9518623948097229 \n",
      "Epoch:30 Learning rate 0.002825638034449818 Weight Decay 1e-07 Train Loss:0.9408944249153137  Valid Loss:0.9490034580230713 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,None,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9431902766227722  Valid Loss:0.952446460723877 \n",
      "Epoch:1 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9365893006324768  Valid Loss:0.9474843740463257 \n",
      "Epoch:2 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.934906542301178  Valid Loss:0.9458240270614624 \n",
      "Epoch:3 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9338083863258362  Valid Loss:0.9437755942344666 \n",
      "Epoch:4 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9319905638694763  Valid Loss:0.9444485306739807 \n",
      "Epoch:5 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9281965494155884  Valid Loss:0.9406331181526184 \n",
      "Epoch:6 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.927833080291748  Valid Loss:0.9388197064399719 \n",
      "Epoch:7 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9300311803817749  Valid Loss:0.9409380555152893 \n",
      "Epoch:8 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9262545108795166  Valid Loss:0.9402275681495667 \n",
      "Epoch:9 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.928620457649231  Valid Loss:0.9412751197814941 \n",
      "Epoch:10 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9246418476104736  Valid Loss:0.9362256526947021 \n",
      "Epoch:11 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9269962906837463  Valid Loss:0.9403133988380432 \n",
      "Epoch:12 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9241676926612854  Valid Loss:0.9374616146087646 \n",
      "Epoch:13 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9224355816841125  Valid Loss:0.9361529350280762 \n",
      "Epoch:14 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9222636818885803  Valid Loss:0.9351399540901184 \n",
      "Epoch:15 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9237077832221985  Valid Loss:0.9381808638572693 \n",
      "Epoch:16 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9241113662719727  Valid Loss:0.9374433755874634 \n",
      "Epoch:17 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9215826392173767  Valid Loss:0.935041069984436 \n",
      "Epoch:18 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9214653372764587  Valid Loss:0.9363003373146057 \n",
      "Epoch:19 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9198381304740906  Valid Loss:0.9337903261184692 \n",
      "Epoch:20 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9192540049552917  Valid Loss:0.9321796894073486 \n",
      "Epoch:21 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.918315052986145  Valid Loss:0.933958888053894 \n",
      "Epoch:22 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.917314350605011  Valid Loss:0.9333019256591797 \n",
      "Epoch:23 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9181917309761047  Valid Loss:0.9329001903533936 \n",
      "Epoch:24 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9190579056739807  Valid Loss:0.9348559379577637 \n",
      "Epoch:25 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9163161516189575  Valid Loss:0.9327476024627686 \n",
      "Epoch:26 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9149879813194275  Valid Loss:0.9291052222251892 \n",
      "Epoch:27 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9168253540992737  Valid Loss:0.9321165084838867 \n",
      "Epoch:28 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9139019846916199  Valid Loss:0.929658055305481 \n",
      "Epoch:29 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9125166535377502  Valid Loss:0.9282936453819275 \n",
      "Epoch:30 Learning rate 0.001 Weight Decay 1e-07 Train Loss:0.9124282598495483  Valid Loss:0.9307752847671509 \n"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0,start_lr=1e-1,end_lr=5e-2)\n",
    "learner.run_epochs(dltrain,None,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9153982400894165  Valid Loss:0.9308962225914001 \n",
      "Epoch:1 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9140071868896484  Valid Loss:0.9305980205535889 \n",
      "Epoch:2 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9147056937217712  Valid Loss:0.9299253225326538 \n",
      "Epoch:3 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9146891236305237  Valid Loss:0.9302049875259399 \n",
      "Epoch:4 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.913002610206604  Valid Loss:0.9288424253463745 \n",
      "Epoch:5 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9135340452194214  Valid Loss:0.9283024072647095 \n",
      "Epoch:6 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9130706787109375  Valid Loss:0.9293491244316101 \n",
      "Epoch:7 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9131229519844055  Valid Loss:0.9297102093696594 \n",
      "Epoch:8 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9122061729431152  Valid Loss:0.9282559156417847 \n",
      "Epoch:9 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9117133617401123  Valid Loss:0.9273900985717773 \n",
      "Epoch:10 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9137289524078369  Valid Loss:0.9297609329223633 \n",
      "Epoch:11 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9110565185546875  Valid Loss:0.926171600818634 \n",
      "Epoch:12 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9130322933197021  Valid Loss:0.9288447499275208 \n",
      "Epoch:13 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9122596979141235  Valid Loss:0.9273991584777832 \n",
      "Epoch:14 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9128934144973755  Valid Loss:0.9288520216941833 \n",
      "Epoch:15 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9136032462120056  Valid Loss:0.9288490414619446 \n",
      "Epoch:16 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9088056087493896  Valid Loss:0.9217606782913208 \n",
      "Epoch:17 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9097679853439331  Valid Loss:0.9245319962501526 \n",
      "Epoch:18 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9093044996261597  Valid Loss:0.9246109127998352 \n",
      "Epoch:19 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9098809957504272  Valid Loss:0.9251683950424194 \n",
      "Epoch:20 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9070664644241333  Valid Loss:0.9222778081893921 \n",
      "Epoch:21 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9083812832832336  Valid Loss:0.924340546131134 \n",
      "Epoch:22 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9072530269622803  Valid Loss:0.9224819540977478 \n",
      "Epoch:23 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9072640538215637  Valid Loss:0.9242957830429077 \n",
      "Epoch:24 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.908643364906311  Valid Loss:0.9242017865180969 \n",
      "Epoch:25 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9044616222381592  Valid Loss:0.919769287109375 \n",
      "Epoch:26 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9050942659378052  Valid Loss:0.920662522315979 \n",
      "Epoch:27 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9062338471412659  Valid Loss:0.9228624105453491 \n",
      "Epoch:28 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9054678082466125  Valid Loss:0.9215344190597534 \n",
      "Epoch:29 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.907202422618866  Valid Loss:0.9238505959510803 \n",
      "Epoch:30 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9051908254623413  Valid Loss:0.9220010638237 \n"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=1e-6)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0,start_lr=1e-1,end_lr=5e-2)\n",
    "learner.run_epochs(dltrain,None,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.917055070400238  Valid Loss:0.9278200268745422 \n",
      "Epoch:1 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9119176864624023  Valid Loss:0.9232094883918762 \n",
      "Epoch:2 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9112654328346252  Valid Loss:0.922114908695221 \n",
      "Epoch:3 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9109389185905457  Valid Loss:0.9235572218894958 \n",
      "Epoch:4 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9103987216949463  Valid Loss:0.9232545495033264 \n",
      "Epoch:5 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9056656956672668  Valid Loss:0.9183350205421448 \n",
      "Epoch:6 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9061340093612671  Valid Loss:0.9198451638221741 \n",
      "Epoch:7 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9060786366462708  Valid Loss:0.9210558533668518 \n",
      "Epoch:8 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9032335877418518  Valid Loss:0.9155929684638977 \n",
      "Epoch:9 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9045836925506592  Valid Loss:0.9179216623306274 \n",
      "Epoch:10 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9022050499916077  Valid Loss:0.9164303541183472 \n",
      "Epoch:11 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9049230813980103  Valid Loss:0.9189628958702087 \n",
      "Epoch:12 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9049310088157654  Valid Loss:0.9196341037750244 \n",
      "Epoch:13 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9029020071029663  Valid Loss:0.9177064299583435 \n",
      "Epoch:14 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8992434144020081  Valid Loss:0.9154347777366638 \n",
      "Epoch:15 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9002657532691956  Valid Loss:0.9164158701896667 \n",
      "Epoch:16 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8981285095214844  Valid Loss:0.9141682982444763 \n",
      "Epoch:17 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8998580574989319  Valid Loss:0.9151825904846191 \n",
      "Epoch:18 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8958187699317932  Valid Loss:0.9120003581047058 \n",
      "Epoch:19 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.9001188278198242  Valid Loss:0.9165908098220825 \n",
      "Epoch:20 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8986364603042603  Valid Loss:0.9156107306480408 \n",
      "Epoch:21 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8964515924453735  Valid Loss:0.9119899272918701 \n",
      "Epoch:22 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8972855806350708  Valid Loss:0.9143189787864685 \n",
      "Epoch:23 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8943192362785339  Valid Loss:0.9128122925758362 \n",
      "Epoch:24 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8959283232688904  Valid Loss:0.9126508235931396 \n",
      "Epoch:25 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8952004909515381  Valid Loss:0.9116678237915039 \n",
      "Epoch:26 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.896473228931427  Valid Loss:0.9143642783164978 \n",
      "Epoch:27 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8945231437683105  Valid Loss:0.9104066491127014 \n",
      "Epoch:28 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8937444090843201  Valid Loss:0.9118208885192871 \n",
      "Epoch:29 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8932659029960632  Valid Loss:0.9137946963310242 \n",
      "Epoch:30 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8910592198371887  Valid Loss:0.910490870475769 \n",
      "Epoch:31 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8944703340530396  Valid Loss:0.9123837351799011 \n",
      "Epoch:32 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8941125273704529  Valid Loss:0.9143288731575012 \n",
      "Epoch:33 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8939207196235657  Valid Loss:0.9122085571289062 \n",
      "Epoch:34 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8925817012786865  Valid Loss:0.9122132658958435 \n",
      "Epoch:35 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8970043063163757  Valid Loss:0.914537250995636 \n",
      "Epoch:36 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8929020166397095  Valid Loss:0.9131842255592346 \n",
      "Epoch:37 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8937053084373474  Valid Loss:0.9117209315299988 \n",
      "Epoch:38 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.890903890132904  Valid Loss:0.9080582857131958 \n",
      "Epoch:39 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8929484486579895  Valid Loss:0.9119323492050171 \n",
      "Epoch:40 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8927624225616455  Valid Loss:0.9098324775695801 \n",
      "Epoch:41 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8896625638008118  Valid Loss:0.9079926013946533 \n",
      "Epoch:42 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8905444145202637  Valid Loss:0.9101157188415527 \n",
      "Epoch:43 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8905534148216248  Valid Loss:0.911086916923523 \n",
      "Epoch:44 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8915182948112488  Valid Loss:0.9100626707077026 \n",
      "Epoch:45 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8910865187644958  Valid Loss:0.9103895425796509 \n",
      "Epoch:46 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.888498842716217  Valid Loss:0.9094150066375732 \n",
      "Epoch:47 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8893481492996216  Valid Loss:0.9092868566513062 \n",
      "Epoch:48 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.890127956867218  Valid Loss:0.9110299348831177 \n",
      "Epoch:49 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.890607476234436  Valid Loss:0.9093050956726074 \n",
      "Epoch:50 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8908580541610718  Valid Loss:0.9120479226112366 \n",
      "Epoch:51 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8901843428611755  Valid Loss:0.9105990529060364 \n",
      "Epoch:52 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8908423781394958  Valid Loss:0.9114270806312561 \n",
      "Epoch:53 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8894708156585693  Valid Loss:0.9106741547584534 \n",
      "Epoch:54 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8890611529350281  Valid Loss:0.9096256494522095 \n",
      "Epoch:55 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8898345232009888  Valid Loss:0.9096754789352417 \n",
      "Epoch:56 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8878468871116638  Valid Loss:0.9083833694458008 \n",
      "Epoch:57 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8867153525352478  Valid Loss:0.9063445329666138 \n",
      "Epoch:58 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8890811800956726  Valid Loss:0.9095504879951477 \n",
      "Epoch:59 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8878005743026733  Valid Loss:0.9089025259017944 \n",
      "Epoch:60 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.889435350894928  Valid Loss:0.9107412099838257 \n",
      "Epoch:61 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8902033567428589  Valid Loss:0.9105721712112427 \n",
      "Epoch:62 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8898285031318665  Valid Loss:0.911163866519928 \n",
      "Epoch:63 Learning rate 0.001 Weight Decay 1e-06 Train Loss:0.8870139122009277  Valid Loss:0.9094294309616089 \n"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=1e-3,betas=(0.9,0.999), weight_decay=1e-6)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0,start_lr=1e-1,end_lr=5e-2)\n",
    "learner.run_epochs(dltrain,None,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8885214328765869  Valid Loss:0.9098312854766846 \n",
      "Epoch:1 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8867849111557007  Valid Loss:0.9087008833885193 \n",
      "Epoch:2 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8851664662361145  Valid Loss:0.9070160388946533 \n",
      "Epoch:3 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.885979175567627  Valid Loss:0.9071924090385437 \n",
      "Epoch:4 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.885290265083313  Valid Loss:0.9082332253456116 \n",
      "Epoch:5 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8859005570411682  Valid Loss:0.909111499786377 \n",
      "Epoch:6 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8841961026191711  Valid Loss:0.9063416123390198 \n",
      "Epoch:7 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8863221406936646  Valid Loss:0.9091119766235352 \n",
      "Epoch:8 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8846147060394287  Valid Loss:0.9079487323760986 \n",
      "Epoch:9 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.883080005645752  Valid Loss:0.9059152603149414 \n",
      "Epoch:10 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8814732432365417  Valid Loss:0.9051494598388672 \n",
      "Epoch:11 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8831799030303955  Valid Loss:0.9057688117027283 \n",
      "Epoch:12 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8850975036621094  Valid Loss:0.9076365828514099 \n",
      "Epoch:13 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8831771612167358  Valid Loss:0.9071827530860901 \n",
      "Epoch:14 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8825194239616394  Valid Loss:0.9048829078674316 \n",
      "Epoch:15 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8857542872428894  Valid Loss:0.9109192490577698 \n",
      "Epoch:16 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8830521702766418  Valid Loss:0.9068478941917419 \n",
      "Epoch:17 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.88311368227005  Valid Loss:0.9070664644241333 \n",
      "Epoch:18 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8830968141555786  Valid Loss:0.9060748815536499 \n",
      "Epoch:19 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8836994171142578  Valid Loss:0.9051233530044556 \n",
      "Epoch:20 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8842355608940125  Valid Loss:0.906997799873352 \n",
      "Epoch:21 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8823974132537842  Valid Loss:0.9049848914146423 \n",
      "Epoch:22 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8815966844558716  Valid Loss:0.90582674741745 \n",
      "Epoch:23 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.882690966129303  Valid Loss:0.9062641263008118 \n",
      "Epoch:24 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8839765787124634  Valid Loss:0.9073293209075928 \n",
      "Epoch:25 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8821322321891785  Valid Loss:0.9048506021499634 \n",
      "Epoch:26 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8819088339805603  Valid Loss:0.9050935506820679 \n",
      "Epoch:27 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8820191621780396  Valid Loss:0.9044961929321289 \n",
      "Epoch:28 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8821423053741455  Valid Loss:0.9061104655265808 \n",
      "Epoch:29 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8838066458702087  Valid Loss:0.9099463224411011 \n",
      "Epoch:30 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8815471529960632  Valid Loss:0.9059726595878601 \n",
      "Epoch:31 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8828572034835815  Valid Loss:0.9041696786880493 \n",
      "Epoch:32 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8818833827972412  Valid Loss:0.9062405824661255 \n",
      "Epoch:33 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8815606236457825  Valid Loss:0.9056504368782043 \n",
      "Epoch:34 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8824325203895569  Valid Loss:0.9049779176712036 \n",
      "Epoch:35 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8822013735771179  Valid Loss:0.9073776602745056 \n",
      "Epoch:36 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8811315298080444  Valid Loss:0.9039095044136047 \n",
      "Epoch:37 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.881435215473175  Valid Loss:0.905479371547699 \n",
      "Epoch:38 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8791244626045227  Valid Loss:0.9026709198951721 \n",
      "Epoch:39 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8822880983352661  Valid Loss:0.904414176940918 \n",
      "Epoch:40 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8804397583007812  Valid Loss:0.905643880367279 \n",
      "Epoch:41 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8830682635307312  Valid Loss:0.9083372950553894 \n",
      "Epoch:42 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8813049793243408  Valid Loss:0.9043071866035461 \n",
      "Epoch:43 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8824960589408875  Valid Loss:0.9087256789207458 \n",
      "Epoch:44 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8799100518226624  Valid Loss:0.9043323993682861 \n",
      "Epoch:45 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8843386173248291  Valid Loss:0.9083980917930603 \n",
      "Epoch:46 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8798649311065674  Valid Loss:0.9058815240859985 \n",
      "Epoch:47 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8822527527809143  Valid Loss:0.9055320024490356 \n",
      "Epoch:48 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8809860944747925  Valid Loss:0.9055728316307068 \n",
      "Epoch:49 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8795921206474304  Valid Loss:0.9031835198402405 \n",
      "Epoch:50 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8823879957199097  Valid Loss:0.9055627584457397 \n",
      "Epoch:51 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8799942135810852  Valid Loss:0.9038264155387878 \n",
      "Epoch:52 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.883158802986145  Valid Loss:0.9086648225784302 \n",
      "Epoch:53 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8819523453712463  Valid Loss:0.9095438718795776 \n",
      "Epoch:54 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8811196088790894  Valid Loss:0.9067757725715637 \n",
      "Epoch:55 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8800986409187317  Valid Loss:0.9040384888648987 \n",
      "Epoch:56 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8807601928710938  Valid Loss:0.9056361317634583 \n",
      "Epoch:57 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8823175430297852  Valid Loss:0.9070408344268799 \n",
      "Epoch:58 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8792331218719482  Valid Loss:0.9064574837684631 \n",
      "Epoch:59 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8790610432624817  Valid Loss:0.9050001502037048 \n",
      "Epoch:60 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8817335963249207  Valid Loss:0.9079742431640625 \n",
      "Epoch:61 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.88081294298172  Valid Loss:0.9042533040046692 \n",
      "Epoch:62 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8790930509567261  Valid Loss:0.9038391709327698 \n",
      "Epoch:63 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8790830373764038  Valid Loss:0.9041685461997986 \n",
      "Epoch:64 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8791497945785522  Valid Loss:0.9032502174377441 \n",
      "Epoch:65 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8810017108917236  Valid Loss:0.9057633280754089 \n",
      "Epoch:66 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8817036151885986  Valid Loss:0.9067282676696777 \n",
      "Epoch:67 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8803344368934631  Valid Loss:0.906038224697113 \n",
      "Epoch:68 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8809576630592346  Valid Loss:0.9052069783210754 \n",
      "Epoch:69 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8796246647834778  Valid Loss:0.9043994545936584 \n",
      "Epoch:70 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8829102516174316  Valid Loss:0.9066531658172607 \n",
      "Epoch:71 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8815138339996338  Valid Loss:0.9088475704193115 \n",
      "Epoch:72 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8804029822349548  Valid Loss:0.9057633876800537 \n",
      "Epoch:73 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8824799060821533  Valid Loss:0.9069294929504395 \n",
      "Epoch:74 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8786250352859497  Valid Loss:0.9035993814468384 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:75 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8801370859146118  Valid Loss:0.9058658480644226 \n",
      "Epoch:76 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8820619583129883  Valid Loss:0.9068834185600281 \n",
      "Epoch:77 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8807319402694702  Valid Loss:0.9058859348297119 \n",
      "Epoch:78 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8801190257072449  Valid Loss:0.906078577041626 \n",
      "Epoch:79 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.879687488079071  Valid Loss:0.9043372869491577 \n",
      "Epoch:80 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8799718618392944  Valid Loss:0.9055473208427429 \n",
      "Epoch:81 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.879746675491333  Valid Loss:0.9045162200927734 \n",
      "Epoch:82 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8789826035499573  Valid Loss:0.9043486714363098 \n",
      "Epoch:83 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8804485201835632  Valid Loss:0.9040932059288025 \n",
      "Epoch:84 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8792825937271118  Valid Loss:0.903448224067688 \n",
      "Epoch:85 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8787016272544861  Valid Loss:0.9053761959075928 \n",
      "Epoch:86 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8798254728317261  Valid Loss:0.9053288102149963 \n",
      "Epoch:87 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.878812313079834  Valid Loss:0.9059132933616638 \n",
      "Epoch:88 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8779826760292053  Valid Loss:0.9033438563346863 \n",
      "Epoch:89 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8789689540863037  Valid Loss:0.9044524431228638 \n",
      "Epoch:90 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8796708583831787  Valid Loss:0.9064071774482727 \n",
      "Epoch:91 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.87886643409729  Valid Loss:0.9056397676467896 \n",
      "Epoch:92 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8798977732658386  Valid Loss:0.9051510095596313 \n",
      "Epoch:93 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8788787126541138  Valid Loss:0.9045067429542542 \n",
      "Epoch:94 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8774005174636841  Valid Loss:0.903261661529541 \n",
      "Epoch:95 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.881328821182251  Valid Loss:0.907515823841095 \n",
      "Epoch:96 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8786861300468445  Valid Loss:0.9049271941184998 \n",
      "Epoch:97 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.880437433719635  Valid Loss:0.9066919684410095 \n",
      "Epoch:98 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8780564665794373  Valid Loss:0.9035189151763916 \n",
      "Epoch:99 Learning rate 0.0007 Weight Decay 1e-06 Train Loss:0.8792470693588257  Valid Loss:0.9049305319786072 \n"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=7e-4,betas=(0.9,0.999), weight_decay=1e-6)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0,start_lr=1e-1,end_lr=5e-2)\n",
    "learner.run_epochs(dltrain,None,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirana/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type autoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kirana/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type autoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(autoenc,f=f'{DATAPATH}/inter/user_autoenc')\n",
    "torch.save(learner,f=f'{DATAPATH}/inter/user_autoenc_learner')\n",
    "torch.save(optimizer,f=f'{DATAPATH}/inter/user_autoenc_optimizer')\n",
    "torch.save(autoenc.state_dict(),f'{DATAPATH}/inter/user_autoenc_state_dict')\n",
    "torch.save(optimizer.state_dict(),f'{DATAPATH}/inter/user_autoenc_optimizer_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 3707])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[0][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 40])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[1][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mtx_1_weights=autoenc.encoder[0][0].weight.data.cpu().numpy()\n",
    "user_mtx_2_weights=autoenc.encoder[1][0].weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train_user_autoenc=np.tanh(expit(df_train@user_mtx_1_weights.T)@user_mtx_2_weights.T)\n",
    "df_valid_user_autoenc=np.tanh(expit(df_valid@user_mtx_1_weights.T)@user_mtx_2_weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc=(expit(df_train@user_mtx_1_weights.T)@user_mtx_2_weights.T)\n",
    "df_valid_user_autoenc=(expit(df_valid@user_mtx_1_weights.T)@user_mtx_2_weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc_linear=((df_train@user_mtx_1_weights.T)@user_mtx_2_weights.T)\n",
    "df_valid_user_autoenc_linear=((df_valid@user_mtx_1_weights.T)@user_mtx_2_weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6041, 10), (6041, 10))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_user_autoenc.shape,df_valid_user_autoenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc=pd.DataFrame(df_train_user_autoenc)\n",
    "df_train_user_autoenc_linear=pd.DataFrame(df_train_user_autoenc_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc.columns=['user_autoenc'+str(i) for i in range(df_train_user_autoenc.shape[1])]\n",
    "df_train_user_autoenc_linear.columns=['user_autoenc_linear'+str(i) for i in range(df_train_user_autoenc_linear.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_user_autoenc=pd.DataFrame(df_valid_user_autoenc)\n",
    "df_valid_user_autoenc_linear=pd.DataFrame(df_valid_user_autoenc_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_user_autoenc.columns=['user_autoenc'+str(i) for i in range(df_valid_user_autoenc.shape[1])]\n",
    "df_valid_user_autoenc_linear.columns=['user_autoenc_linear'+str(i) for i in range(df_valid_user_autoenc_linear.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc.reset_index(inplace=True)\n",
    "df_valid_user_autoenc.reset_index(inplace=True)\n",
    "df_train_user_autoenc_linear.reset_index(inplace=True)\n",
    "df_valid_user_autoenc_linear.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc.rename({'index':'user_idx'},axis=1,inplace=True)\n",
    "df_train_user_autoenc_linear.rename({'index':'user_idx'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_user_autoenc.rename({'index':'user_idx'},axis=1,inplace=True)\n",
    "df_valid_user_autoenc_linear.rename({'index':'user_idx'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   user_idx  user_autoenc0  user_autoenc1  user_autoenc2  user_autoenc3  \\\n",
       " 0         0      -0.918095       2.296391       0.263371      -0.011968   \n",
       " 1         1      -1.694891      -0.151930       1.124974      -1.841133   \n",
       " 2         2       0.807896      -1.376931       0.162299       0.080092   \n",
       " 3         3      -0.736317      -0.517018       1.213105      -1.961564   \n",
       " 4         4      -1.399151       0.328619      -1.665964      -1.062431   \n",
       " \n",
       "    user_autoenc4  user_autoenc5  user_autoenc6  user_autoenc7  user_autoenc8  \\\n",
       " 0      -0.804040      -0.389131       0.454120      -3.545166       2.439914   \n",
       " 1      -2.112378      -1.080319       1.374260      -2.909504      -0.340611   \n",
       " 2       0.537038       0.585960      -0.161436       1.048445      -1.035838   \n",
       " 3      -1.423592      -1.065770       0.938449      -1.570770      -0.542528   \n",
       " 4      -0.760788       0.939962      -1.311538      -0.389501       0.414060   \n",
       " \n",
       "    user_autoenc9  \n",
       " 0       1.331096  \n",
       " 1      -1.397754  \n",
       " 2      -0.490745  \n",
       " 3      -1.756809  \n",
       " 4      -0.095679  ,\n",
       "    user_idx  user_autoenc_linear0  user_autoenc_linear1  user_autoenc_linear2  \\\n",
       " 0         0             -7.903108             67.759951             -7.420140   \n",
       " 1         1             -2.802475              0.424496             16.149818   \n",
       " 2         2              8.951077            -22.260454            -11.214062   \n",
       " 3         3             -1.261564              3.822455             13.205763   \n",
       " 4         4              5.547645             20.531805            -20.436249   \n",
       " \n",
       "    user_autoenc_linear3  user_autoenc_linear4  user_autoenc_linear5  \\\n",
       " 0             28.591522             -5.811607            -40.421312   \n",
       " 1             -2.495744            -12.774612            -39.868628   \n",
       " 2              0.865454              5.211422            -28.063715   \n",
       " 3             -9.010028            -12.793017            -37.472720   \n",
       " 4             28.186211             13.495807            -15.872422   \n",
       " \n",
       "    user_autoenc_linear6  user_autoenc_linear7  user_autoenc_linear8  \\\n",
       " 0             -0.150670            -67.959909             67.728086   \n",
       " 1             14.219897            -22.102997             -0.916217   \n",
       " 2            -17.644808             24.324165            -19.604151   \n",
       " 3              8.642195            -17.388834              2.867866   \n",
       " 4            -19.256115             -4.841874             22.929547   \n",
       " \n",
       "    user_autoenc_linear9  \n",
       " 0             51.638784  \n",
       " 1             -5.332345  \n",
       " 2            -10.153207  \n",
       " 3             -8.316666  \n",
       " 4             31.481783  )"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_user_autoenc.head(),df_train_user_autoenc_linear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([df_train_user_autoenc,df_valid_user_autoenc,user_mtx_1_weights,user_mtx_2_weights,df_train_user_autoenc_linear,df_valid_user_autoenc_linear],open(f'{DATAPATH}/inter/user_autoenc_weights.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#,df_train_user_autoenc_linear,df_valid_user_autoenc_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=3707, out_features=40, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.2)\n",
       "    (3): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=10, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.2)\n",
       "    (3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6041, 11)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_user_autoenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
