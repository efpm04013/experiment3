{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/home/kirana/Documents/phd/exp3_autoencoder'\n",
    "DATAPATH='/home/kirana/Documents/final_dissertation_final/experiments/datasets/ml-latest-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df, df_train,df_valid,df,df_ratings,dfflagtrain,dfflagvalid,idx_to_user,\\\n",
    "             idx_to_movie,movie_to_idx,user_to_idx]=pickle.load(open(f'{DATAPATH}/reads.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 610)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencdata (Dataset):\n",
    "    def __init__(self,dfX,dfXv):\n",
    "        self.dfX,self.dfXv=dfX,dfXv\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.dfX.shape[0]\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        return torch.FloatTensor(self.dfX.iloc[idx].values),torch.FloatTensor(self.dfXv.iloc[idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=autoencdata(df_train, df_valid)\n",
    "#dsvalid=autoencdata(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader object\n",
    "dltrain=DataLoader(dstrain,batch_size=bs,shuffle=False)\n",
    "#dlvalid=DataLoader(dsvalid,batch_size=bs,shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 5.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].min(),df['rating'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model Architecture for the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(x,y,dropout,activation=nn.Sigmoid()):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(x, y),\n",
    "        activation,\n",
    "        nn.Dropout(p=dropout)\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder (nn.Module):    \n",
    "    def __init__(self,n_inp=9724,hidden=[50,10],dropouts=[0,0,0],rating_range=[0.5,5]):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.hidden,self.dropouts,self.rating_range=n_inp,hidden,dropouts,rating_range\n",
    "        encoder=[hidden_layer(n_inp if i==0 else hidden[i-1],hidden[i],dropouts[i],\\\n",
    "                              nn.Sigmoid() if i<len(hidden)-1 else nn.Tanh()) for i in range(len(hidden))]\n",
    "        self.encoder=nn.Sequential(*encoder)\n",
    "        hidden=hidden[::-1]\n",
    "        num_steps=len(hidden)-1\n",
    "        dropouts=dropouts[num_steps:]\n",
    "        decoder=[hidden_layer(hidden[i],hidden[i+1] if i<len(hidden)-1 else n_inp,dropouts[i]) for i in range(len(hidden)-1)]\n",
    "        self.decoder=nn.Sequential(*decoder)\n",
    "        self.fc=nn.Linear(hidden[-1],n_inp)\n",
    "        self.initialize()\n",
    "        self.criterion=nn.MSELoss()\n",
    "    \n",
    "    def initialize(self):\n",
    "        for x in self.encoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "        for x in self.decoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "\n",
    "    def forward (self,Xb):\n",
    "        encoded=self.encoder(Xb)\n",
    "        decoded=self.decoder(encoded)\n",
    "        out=self.fc(decoded)\n",
    "        outv=out.clone()\n",
    "        out[Xb==0]=0\n",
    "        loss=self.criterion(out,Xb)\n",
    "        return outv,loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[50,10],[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=9724, out_features=50, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=50, out_features=9724, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 0., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for Xb,Xb_v in dltrain:\n",
    "    print (Xb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 9724])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 0., 4.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,loss,_=autoenc.forward(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 9724])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.MSELoss"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-4\n",
    "#wd=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "#optimizer=torch.optim.SGD(model_sentiment.parameters(),lr=1e-2,momentum=0.9, weight_decay=wd)\n",
    "metric_fn=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dltrain.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None,\\\n",
    "                 cycle_mult=0,lr_decay=0.7,wd_mult=6,start_lr=2e-2, end_lr=5e-4):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "        self.cycle_mult,self.lr_decay=cycle_mult,lr_decay\n",
    "        self.wd_mult=wd_mult\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            self.start_lr=param_group['lr']\n",
    "            self.start_wd=param_group['weight_decay']\n",
    "        self.wd=self.start_wd\n",
    "        self.lr=self.start_lr\n",
    "        self.end_lr=end_lr\n",
    "        self.n_epoch=0\n",
    "        self.lrs=[1e-2,5e-3,1e-4,5e-4]\n",
    "        self.preds,self.preds_valid,self.trainY,self.actual=[],[],[],[]\n",
    "        self.ratio=self.end_lr/self.start_lr\n",
    "        self.num_steps=self.cycle_mult\n",
    "        self.reset_cycle=self.cycle_mult\n",
    "        \n",
    "    def fit (self,Xb,Xb_v,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        \n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        preds,loss,preds_train=self.model(Xb)\n",
    "        # denominator is the average of the error with non-zero ratings\n",
    "\n",
    "        mean_corrector = Xb.size(0)*Xb.size(1)/(torch.sum(Xb > 0).float() + 1e-10)\n",
    "        mean_corrector_v = Xb_v.size(0)*Xb_v.size(1)/(torch.sum(Xb_v > 0).float() + 1e-10)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds[Xb_v==0]=0\n",
    "            loss_v=self.model.criterion(preds,Xb_v)\n",
    "            \n",
    "            if self.metric_fn is not None:\n",
    "                acc=self.metric_fn(preds,Yb.view(-1),self.device)\n",
    "                acc=acc.item()\n",
    "\n",
    "                if 1==0:\n",
    "                    if mode_train:\n",
    "                        self.trainY.append(Yb.view(-1))\n",
    "                        self.preds.append(preds.data)\n",
    "                    else:\n",
    "                        self.actual.append(Yb.view(-1))\n",
    "                        self.preds_valid.append(preds.data)\n",
    "            else:\n",
    "                acc=0\n",
    "                acc_v=0\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            if 1==0:\n",
    "                lr =self.lrs[torch.randint(0,4,(1,))]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=torch.sqrt(loss.item()*mean_corrector)\n",
    "        myloss_v=torch.sqrt(loss_v.item()*mean_corrector_v)\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)\n",
    "        \n",
    "        return myloss, acc,myloss_v,acc_v\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        epoch_loss_v,epoch_acc_v=0,0\n",
    "\n",
    "        for Xb,Xb_v in iterator:\n",
    "            Xb=Xb.to(self.device)\n",
    "            Xb_v=Xb_v.to(self.device)\n",
    "            #Xb=Xb.squeeze(0)\n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc,loss_v,acc_v=self.fit(Xb,Xb_v,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            epoch_loss_v+=loss_v\n",
    "            epoch_acc_v+=acc_v\n",
    "            \n",
    "            k=k+1\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)} {epoch_loss_v/(k)} ')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/len(iterator)\n",
    "        epoch_acc=epoch_acc/len(iterator)\n",
    "        epoch_loss_v=epoch_loss_v/len(iterator)\n",
    "        epoch_acc_v=epoch_acc_v/len(iterator)\n",
    "            \n",
    "        return epoch_loss,epoch_acc,epoch_loss_v,epoch_acc_v\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1,ylim=None,xlim=None):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "        if xlim is not None:\n",
    "            plt.xlim(xlim)\n",
    "\n",
    "     \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        for epoch in range(n_epochs):                \n",
    "\n",
    "            loss,acc,lossv,accv=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Learning rate {self.lr} Weight Decay {self.wd} Train Loss:{loss}  Valid Loss:{lossv} ')\n",
    "  \n",
    "            if self.cycle_mult:\n",
    "                if self.n_epoch==self.reset_cycle:\n",
    "                    self.lr=self.start_lr\n",
    "                    #self.wd=self.start_wd\n",
    "                    self.reset_cycle*=self.cycle_mult\n",
    "                    #reset_cycle=self.n_epoch+reset_cycle\n",
    "                    self.n_epoch=0\n",
    "                    self.ratio=self.end_lr/self.start_lr\n",
    "                    self.num_steps=self.reset_cycle\n",
    "                else:\n",
    "                    #self.lr*=(self.lr_decay**self.n_epoch)  \n",
    "                    #if self.n_epoch>1:\n",
    "                    #    self.wd*=self.wd_mult\n",
    "                    self.lr=self.lr*(self.end_lr/self.start_lr)**(1/self.num_steps)\n",
    "                    self.n_epoch+=1\n",
    "        \n",
    "\n",
    "                \n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr']=self.lr\n",
    "                #param_group['weight_decay']=self.wd\n",
    "          \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[20,10],[0.6,0.6,0]).to(device)\n",
    "wd=1e-7\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=9724, out_features=20, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=20, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=20, out_features=9724, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(1e-4,1e-1,dltrain,len(dltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plot_lrs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[20,10],[0.25,0.25,0]).to(device)\n",
    "wd=1e-7\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=3e-2,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=2,start_lr=3e-2,end_lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.8628110885620117  Valid Loss:1.8781671524047852 \n",
      "Epoch:1 Learning rate 0.0038729833462074164 Weight Decay 1e-07 Train Loss:1.16106116771698  Valid Loss:1.314086675643921 \n",
      "Epoch:2 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:1.0906704664230347  Valid Loss:1.2674089670181274 \n",
      "Epoch:3 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.215004324913025  Valid Loss:1.3479294776916504 \n",
      "Epoch:4 Learning rate 0.010779123358892525 Weight Decay 1e-07 Train Loss:1.110046625137329  Valid Loss:1.2815836668014526 \n",
      "Epoch:5 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:1.0262668132781982  Valid Loss:1.2425158023834229 \n",
      "Epoch:6 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.9995096325874329  Valid Loss:1.2228466272354126 \n",
      "Epoch:7 Learning rate 0.0004999999999999998 Weight Decay 1e-07 Train Loss:1.0086957216262817  Valid Loss:1.2374827861785889 \n",
      "Epoch:8 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.1396219730377197  Valid Loss:1.2950224876403809 \n",
      "Epoch:9 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:1.1304583549499512  Valid Loss:1.2873916625976562 \n",
      "Epoch:10 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:1.058943748474121  Valid Loss:1.2426315546035767 \n",
      "Epoch:11 Learning rate 0.006461220105808663 Weight Decay 1e-07 Train Loss:1.032928705215454  Valid Loss:1.2504210472106934 \n",
      "Epoch:12 Learning rate 0.0038729833462074173 Weight Decay 1e-07 Train Loss:1.0035959482192993  Valid Loss:1.221596360206604 \n",
      "Epoch:13 Learning rate 0.0023215429523156072 Weight Decay 1e-07 Train Loss:1.0051008462905884  Valid Loss:1.2218390703201294 \n",
      "Epoch:14 Learning rate 0.0013915788418568708 Weight Decay 1e-07 Train Loss:0.9907482266426086  Valid Loss:1.22256338596344 \n",
      "Epoch:15 Learning rate 0.0008341399288659162 Weight Decay 1e-07 Train Loss:0.9967285394668579  Valid Loss:1.2268004417419434 \n",
      "Epoch:16 Learning rate 0.0005000000000000002 Weight Decay 1e-07 Train Loss:1.0036119222640991  Valid Loss:1.2294921875 \n",
      "Epoch:17 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.1418404579162598  Valid Loss:1.31475830078125 \n",
      "Epoch:18 Learning rate 0.02322666208281794 Weight Decay 1e-07 Train Loss:1.1208727359771729  Valid Loss:1.2767078876495361 \n",
      "Epoch:19 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:1.0960674285888672  Valid Loss:1.2666078805923462 \n",
      "Epoch:20 Learning rate 0.013922521437378356 Weight Decay 1e-07 Train Loss:1.0493298768997192  Valid Loss:1.249358057975769 \n",
      "Epoch:21 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:1.0522445440292358  Valid Loss:1.2517964839935303 \n",
      "Epoch:22 Learning rate 0.00834543519353354 Weight Decay 1e-07 Train Loss:1.030455470085144  Valid Loss:1.2477924823760986 \n",
      "Epoch:23 Learning rate 0.006461220105808662 Weight Decay 1e-07 Train Loss:1.005704402923584  Valid Loss:1.2270361185073853 \n",
      "Epoch:24 Learning rate 0.005002419201344232 Weight Decay 1e-07 Train Loss:0.98879075050354  Valid Loss:1.2163273096084595 \n",
      "Epoch:25 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:0.9928192496299744  Valid Loss:1.2320897579193115 \n",
      "Epoch:26 Learning rate 0.002998549181158038 Weight Decay 1e-07 Train Loss:0.9713526964187622  Valid Loss:1.2023447751998901 \n",
      "Epoch:27 Learning rate 0.002321542952315606 Weight Decay 1e-07 Train Loss:0.959498405456543  Valid Loss:1.1890443563461304 \n",
      "Epoch:28 Learning rate 0.00179738978880607 Weight Decay 1e-07 Train Loss:0.9831746816635132  Valid Loss:1.2222930192947388 \n",
      "Epoch:29 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.9544939994812012  Valid Loss:1.2004508972167969 \n",
      "Epoch:30 Learning rate 0.0010773910507136221 Weight Decay 1e-07 Train Loss:0.9507237076759338  Valid Loss:1.199827790260315 \n",
      "Epoch:31 Learning rate 0.0008341399288659156 Weight Decay 1e-07 Train Loss:0.9547065496444702  Valid Loss:1.2063637971878052 \n",
      "Epoch:32 Learning rate 0.0006458095419184806 Weight Decay 1e-07 Train Loss:0.9515931010246277  Valid Loss:1.1946979761123657 \n",
      "Epoch:33 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:0.9482910633087158  Valid Loss:1.196582555770874 \n",
      "Epoch:34 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.1014689207077026  Valid Loss:1.2906150817871094 \n",
      "Epoch:35 Learning rate 0.026396966918275633 Weight Decay 1e-07 Train Loss:1.1197577714920044  Valid Loss:1.2734419107437134 \n",
      "Epoch:36 Learning rate 0.02322666208281794 Weight Decay 1e-07 Train Loss:1.1383018493652344  Valid Loss:1.29673433303833 \n",
      "Epoch:37 Learning rate 0.020437114354070404 Weight Decay 1e-07 Train Loss:1.0586919784545898  Valid Loss:1.2532280683517456 \n",
      "Epoch:38 Learning rate 0.017982594383647084 Weight Decay 1e-07 Train Loss:1.0567609071731567  Valid Loss:1.2505747079849243 \n",
      "Epoch:39 Learning rate 0.015822864968330044 Weight Decay 1e-07 Train Loss:1.0170658826828003  Valid Loss:1.226872444152832 \n",
      "Epoch:40 Learning rate 0.013922521437378352 Weight Decay 1e-07 Train Loss:1.0094131231307983  Valid Loss:1.2147083282470703 \n",
      "Epoch:41 Learning rate 0.012250411260048657 Weight Decay 1e-07 Train Loss:0.9970951676368713  Valid Loss:1.2168482542037964 \n",
      "Epoch:42 Learning rate 0.010779123358892525 Weight Decay 1e-07 Train Loss:0.9921311140060425  Valid Loss:1.2023082971572876 \n",
      "Epoch:43 Learning rate 0.009484538757089937 Weight Decay 1e-07 Train Loss:0.9938896298408508  Valid Loss:1.213683843612671 \n",
      "Epoch:44 Learning rate 0.008345435193533538 Weight Decay 1e-07 Train Loss:0.974213719367981  Valid Loss:1.208121418952942 \n",
      "Epoch:45 Learning rate 0.007343139224077267 Weight Decay 1e-07 Train Loss:0.9974167943000793  Valid Loss:1.22245454788208 \n",
      "Epoch:46 Learning rate 0.0064612201058086615 Weight Decay 1e-07 Train Loss:0.961870014667511  Valid Loss:1.1843323707580566 \n",
      "Epoch:47 Learning rate 0.005685220446157621 Weight Decay 1e-07 Train Loss:0.9683899283409119  Valid Loss:1.2066494226455688 \n",
      "Epoch:48 Learning rate 0.005002419201344232 Weight Decay 1e-07 Train Loss:0.9471982717514038  Valid Loss:1.1887967586517334 \n",
      "Epoch:49 Learning rate 0.004401623138974351 Weight Decay 1e-07 Train Loss:0.9447418451309204  Valid Loss:1.182271957397461 \n",
      "Epoch:50 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:0.9446148872375488  Valid Loss:1.1890794038772583 \n",
      "Epoch:51 Learning rate 0.003407833775495654 Weight Decay 1e-07 Train Loss:0.9351191520690918  Valid Loss:1.1826411485671997 \n",
      "Epoch:52 Learning rate 0.002998549181158038 Weight Decay 1e-07 Train Loss:0.9525218605995178  Valid Loss:1.2000576257705688 \n",
      "Epoch:53 Learning rate 0.002638420117928374 Weight Decay 1e-07 Train Loss:0.9530090689659119  Valid Loss:1.1993587017059326 \n",
      "Epoch:54 Learning rate 0.002321542952315606 Weight Decay 1e-07 Train Loss:0.9522924423217773  Valid Loss:1.1993011236190796 \n",
      "Epoch:55 Learning rate 0.0020427230837210333 Weight Decay 1e-07 Train Loss:0.9344910383224487  Valid Loss:1.1833091974258423 \n",
      "Epoch:56 Learning rate 0.00179738978880607 Weight Decay 1e-07 Train Loss:0.9305896162986755  Valid Loss:1.1791293621063232 \n",
      "Epoch:57 Learning rate 0.001581521293145342 Weight Decay 1e-07 Train Loss:0.9325695633888245  Valid Loss:1.181474208831787 \n",
      "Epoch:58 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.9277905821800232  Valid Loss:1.180372953414917 \n",
      "Epoch:59 Learning rate 0.0012244486884222705 Weight Decay 1e-07 Train Loss:0.9247892498970032  Valid Loss:1.1768858432769775 \n",
      "Epoch:60 Learning rate 0.0010773910507136221 Weight Decay 1e-07 Train Loss:0.9120346307754517  Valid Loss:1.1742889881134033 \n",
      "Epoch:61 Learning rate 0.0009479951974577903 Weight Decay 1e-07 Train Loss:0.9250003099441528  Valid Loss:1.1805953979492188 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,None,62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.0008341399288659156 Weight Decay 1e-07 Train Loss:0.9259275794029236  Valid Loss:1.1769760847091675 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 9724])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[0][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[1][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mtx_1_weights=autoenc.encoder[0][0].weight.data.cpu().numpy()\n",
    "user_mtx_2_weights=autoenc.encoder[1][0].weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc=np.tanh(expit(df_train@user_mtx_1_weights.T)@user_mtx_2_weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_user_autoenc=np.tanh(expit(df_valid@user_mtx_1_weights.T)@user_mtx_2_weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((610, 10), (610, 10))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_user_autoenc.shape,df_valid_user_autoenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc.columns=['user_autoenc'+str(i) for i in range(df_train_user_autoenc.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_user_autoenc.columns=['user_autoenc'+str(i) for i in range(df_valid_user_autoenc.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_user_autoenc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_user_autoenc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([df_train_user_autoenc,df_valid_user_autoenc,user_mtx_1_weights,user_mtx_2_weights],open(f'{DATAPATH}/inter/user_autoenc_weights.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=9724, out_features=20, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.25)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.25)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
