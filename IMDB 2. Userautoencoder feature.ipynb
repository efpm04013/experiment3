{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/home/kirana/Documents/phd/exp3_autoencoder'\n",
    "DATAPATH='/home/kirana/Documents/final_dissertation_final/experiments/datasets/ml-latest-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "[df, df_train,df_valid,df,df_ratings,dfflagtrain,dfflagvalid,idx_to_user,\\\n",
    "             idx_to_movie,movie_to_idx,user_to_idx]=pickle.load(open(f'{DATAPATH}/reads.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 610)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencdata (Dataset):\n",
    "    def __init__(self,dfX,dfXv):\n",
    "        self.dfX,self.dfXv=dfX,dfXv\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.dfX.shape[0]\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        return torch.FloatTensor(self.dfX.iloc[idx].values),torch.FloatTensor(self.dfXv.iloc[idx].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain=autoencdata(df_train, df_valid)\n",
    "#dsvalid=autoencdata(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader object\n",
    "dltrain=DataLoader(dstrain,batch_size=bs,shuffle=False)\n",
    "#dlvalid=DataLoader(dsvalid,batch_size=bs,shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 5.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].min(),df['rating'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model Architecture for the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer(x,y,dropout,activation=nn.Sigmoid()):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(x, y),\n",
    "        activation,\n",
    "        nn.Dropout(p=dropout)\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder (nn.Module):    \n",
    "    def __init__(self,n_inp=9724,hidden=[50,10],dropouts=[0,0,0],rating_range=[0.5,5]):\n",
    "        super().__init__()\n",
    "        self.n_inp,self.hidden,self.dropouts,self.rating_range=n_inp,hidden,dropouts,rating_range\n",
    "        encoder=[hidden_layer(n_inp if i==0 else hidden[i-1],hidden[i],dropouts[i],\\\n",
    "                              nn.Sigmoid() if i<len(hidden)-1 else nn.Tanh()) for i in range(len(hidden))]\n",
    "        self.encoder=nn.Sequential(*encoder)\n",
    "        hidden=hidden[::-1]\n",
    "        num_steps=len(hidden)-1\n",
    "        dropouts=dropouts[num_steps:]\n",
    "        decoder=[hidden_layer(hidden[i],hidden[i+1] if i<len(hidden)-1 else n_inp,dropouts[i]) for i in range(len(hidden)-1)]\n",
    "        self.decoder=nn.Sequential(*decoder)\n",
    "        self.fc=nn.Linear(hidden[-1],n_inp)\n",
    "        self.initialize()\n",
    "        self.criterion=nn.MSELoss()\n",
    "    \n",
    "    def initialize(self):\n",
    "        for x in self.encoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "        for x in self.decoder:\n",
    "            nn.init.kaiming_normal_(x[0].weight.data)\n",
    "\n",
    "    def forward (self,Xb):\n",
    "        encoded=self.encoder(Xb)\n",
    "        decoded=self.decoder(encoded)\n",
    "        out=self.fc(decoded)\n",
    "        outv=out.clone()\n",
    "        out[Xb==0]=0\n",
    "        loss=self.criterion(out,Xb)\n",
    "        return outv,loss, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[50,10],[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=9724, out_features=50, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=50, out_features=9724, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 0., 4.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for Xb,Xb_v in dltrain:\n",
    "    print (Xb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 9724])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 0., 4.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,loss,_=autoenc.forward(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 9724])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.MSELoss"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-4\n",
    "#wd=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n",
    "#optimizer=torch.optim.SGD(model_sentiment.parameters(),lr=1e-2,momentum=0.9, weight_decay=wd)\n",
    "metric_fn=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dltrain.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self,model,optimizer,metric_fn,device,bptt=12,print_every=5,clip_val=None,\\\n",
    "                 cycle_mult=0,lr_decay=0.7,wd_mult=6,start_lr=2e-2, end_lr=5e-4):\n",
    "        self.model,self.optimizer,self.metric_fn,self.device,self.print_every,self.bptt,self.losses,self.clip_val=\\\n",
    "            model,optimizer,metric_fn,device,print_every,bptt,[],clip_val\n",
    "        self.n_epochs=1\n",
    "        self.cycle_mult,self.lr_decay=cycle_mult,lr_decay\n",
    "        self.wd_mult=wd_mult\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            self.start_lr=param_group['lr']\n",
    "            self.start_wd=param_group['weight_decay']\n",
    "        self.wd=self.start_wd\n",
    "        self.lr=self.start_lr\n",
    "        self.end_lr=end_lr\n",
    "        self.n_epoch=0\n",
    "        self.lrs=[1e-2,5e-3,1e-4,5e-4]\n",
    "        self.preds,self.preds_valid,self.trainY,self.actual=[],[],[],[]\n",
    "        self.ratio=self.end_lr/self.start_lr\n",
    "        self.num_steps=self.cycle_mult\n",
    "        self.reset_cycle=self.cycle_mult\n",
    "        \n",
    "    def fit (self,Xb,Xb_v,mode_train=True):\n",
    "        if mode_train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        \n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        preds,loss,preds_train=self.model(Xb)\n",
    "        # denominator is the average of the error with non-zero ratings\n",
    "\n",
    "        mean_corrector = Xb.size(0)*Xb.size(1)/(torch.sum(Xb > 0).float() + 1e-10)\n",
    "        mean_corrector_v = Xb_v.size(0)*Xb_v.size(1)/(torch.sum(Xb_v > 0).float() + 1e-10)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds[Xb_v==0]=0\n",
    "            loss_v=self.model.criterion(preds,Xb_v)\n",
    "            \n",
    "            if self.metric_fn is not None:\n",
    "                acc=self.metric_fn(preds,Yb.view(-1),self.device)\n",
    "                acc=acc.item()\n",
    "\n",
    "                if 1==0:\n",
    "                    if mode_train:\n",
    "                        self.trainY.append(Yb.view(-1))\n",
    "                        self.preds.append(preds.data)\n",
    "                    else:\n",
    "                        self.actual.append(Yb.view(-1))\n",
    "                        self.preds_valid.append(preds.data)\n",
    "            else:\n",
    "                acc=0\n",
    "                acc_v=0\n",
    "            del preds\n",
    "        \n",
    "        if mode_train:\n",
    "            if 1==0:\n",
    "                lr =self.lrs[torch.randint(0,4,(1,))]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        myloss=torch.sqrt(loss.item()*mean_corrector)\n",
    "        myloss_v=torch.sqrt(loss_v.item()*mean_corrector_v)\n",
    "        del loss\n",
    "        \n",
    "        if self.clip_val is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)\n",
    "        \n",
    "        return myloss, acc,myloss_v,acc_v\n",
    "    \n",
    "    def lr_find (self,start_lr,end_lr,iterator,n_batch):\n",
    "        losses,lrs=[],[]\n",
    "        ratio=end_lr/start_lr\n",
    "        num_steps=n_batch\n",
    "        lr=start_lr\n",
    "        for i in range(num_steps):            \n",
    "            lr=lr*(end_lr/start_lr)**(1/num_steps)\n",
    "            lrs.append(lr)\n",
    "        self.lrs=lrs\n",
    "        self.run_epoch(iterator,mode_train=True,lrs=lrs)\n",
    "    \n",
    "    def run_epoch(self,iterator,mode_train,lrs=None):\n",
    "        epoch_loss,epoch_acc,i,k=0,0,0,0\n",
    "        epoch_loss_v,epoch_acc_v=0,0\n",
    "\n",
    "        for Xb,Xb_v in iterator:\n",
    "            Xb=Xb.to(self.device)\n",
    "            Xb_v=Xb_v.to(self.device)\n",
    "            #Xb=Xb.squeeze(0)\n",
    "            if lrs is not None:\n",
    "                lr=lrs[k]\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr']=lr \n",
    "            \n",
    "\n",
    "            loss,acc,loss_v,acc_v=self.fit(Xb,Xb_v,mode_train)\n",
    "            \n",
    "            if lrs is not None:\n",
    "                self.losses.append(loss)\n",
    "            \n",
    "            epoch_loss+=loss\n",
    "            epoch_acc+=acc\n",
    "            epoch_loss_v+=loss_v\n",
    "            epoch_acc_v+=acc_v\n",
    "            \n",
    "            k=k+1\n",
    "            if k%self.print_every == 0:\n",
    "                if k:\n",
    "                    print (f'Batch:{k} {epoch_loss/(k)} {epoch_loss_v/(k)} ')  \n",
    "                    torch.cuda.empty_cache()\n",
    "        epoch_loss=epoch_loss/len(iterator)\n",
    "        epoch_acc=epoch_acc/len(iterator)\n",
    "        epoch_loss_v=epoch_loss_v/len(iterator)\n",
    "        epoch_acc_v=epoch_acc_v/len(iterator)\n",
    "            \n",
    "        return epoch_loss,epoch_acc,epoch_loss_v,epoch_acc_v\n",
    "    \n",
    "    def plot_lrs(self, n_roll=1,ylim=None,xlim=None):\n",
    "        import seaborn as sns\n",
    "        ax=sns.lineplot(x=self.lrs,y=pd.Series(self.losses).rolling(n_roll).mean())\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "        if xlim is not None:\n",
    "            plt.xlim(xlim)\n",
    "\n",
    "     \n",
    "    def run_epochs(self,dltrain,dlvalid,n_epochs=1):\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        for epoch in range(n_epochs):                \n",
    "\n",
    "            loss,acc,lossv,accv=self.run_epoch(dltrain,True)\n",
    "            print (f'Epoch:{epoch} Learning rate {self.lr} Weight Decay {self.wd} Train Loss:{loss}  Valid Loss:{lossv} ')\n",
    "  \n",
    "            if self.cycle_mult:\n",
    "                if self.n_epoch==self.reset_cycle:\n",
    "                    self.lr=self.start_lr\n",
    "                    #self.wd=self.start_wd\n",
    "                    self.reset_cycle*=self.cycle_mult\n",
    "                    #reset_cycle=self.n_epoch+reset_cycle\n",
    "                    self.n_epoch=0\n",
    "                    self.ratio=self.end_lr/self.start_lr\n",
    "                    self.num_steps=self.reset_cycle\n",
    "                else:\n",
    "                    #self.lr*=(self.lr_decay**self.n_epoch)  \n",
    "                    #if self.n_epoch>1:\n",
    "                    #    self.wd*=self.wd_mult\n",
    "                    self.lr=self.lr*(self.end_lr/self.start_lr)**(1/self.num_steps)\n",
    "                    self.n_epoch+=1\n",
    "        \n",
    "\n",
    "                \n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr']=self.lr\n",
    "                #param_group['weight_decay']=self.wd\n",
    "          \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[20,10],[0.6,0.6,0]).to(device)\n",
    "wd=1e-7\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=5e-3,betas=(0.9,0.999), weight_decay=wd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=9724, out_features=20, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=20, bias=True)\n",
       "      (1): Sigmoid()\n",
       "      (2): Dropout(p=0.6)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=20, out_features=9724, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(1e-4,1e-1,dltrain,len(dltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.plot_lrs(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc=autoencoder(df_train.shape[1],[20,10],[0.25,0.25,0]).to(device)\n",
    "wd=1e-7\n",
    "optimizer=torch.optim.Adam(autoenc.parameters(),lr=3e-2,betas=(0.9,0.999), weight_decay=wd)\n",
    "learner=Learner(autoenc,optimizer,None,device,0,1000,0.25,cycle_mult=2,start_lr=3e-2,end_lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.0008341399288659156 Weight Decay 1e-07 Train Loss:0.9677209854125977  Valid Loss:1.202816367149353 \n",
      "Epoch:1 Learning rate 0.0006458095419184806 Weight Decay 1e-07 Train Loss:0.9885748624801636  Valid Loss:1.2149444818496704 \n",
      "Epoch:2 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:0.9793049097061157  Valid Loss:1.2153334617614746 \n",
      "Epoch:3 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.0777382850646973  Valid Loss:1.2704707384109497 \n",
      "Epoch:4 Learning rate 0.026396966918275633 Weight Decay 1e-07 Train Loss:1.1677390336990356  Valid Loss:1.311733365058899 \n",
      "Epoch:5 Learning rate 0.02322666208281794 Weight Decay 1e-07 Train Loss:1.146641731262207  Valid Loss:1.319527268409729 \n",
      "Epoch:6 Learning rate 0.020437114354070404 Weight Decay 1e-07 Train Loss:1.0977084636688232  Valid Loss:1.264682412147522 \n",
      "Epoch:7 Learning rate 0.017982594383647084 Weight Decay 1e-07 Train Loss:1.0892280340194702  Valid Loss:1.2754151821136475 \n",
      "Epoch:8 Learning rate 0.015822864968330044 Weight Decay 1e-07 Train Loss:1.0307220220565796  Valid Loss:1.2297900915145874 \n",
      "Epoch:9 Learning rate 0.013922521437378352 Weight Decay 1e-07 Train Loss:1.0262315273284912  Valid Loss:1.227534532546997 \n",
      "Epoch:10 Learning rate 0.012250411260048657 Weight Decay 1e-07 Train Loss:1.0058218240737915  Valid Loss:1.2176527976989746 \n",
      "Epoch:11 Learning rate 0.010779123358892525 Weight Decay 1e-07 Train Loss:1.000322937965393  Valid Loss:1.2088863849639893 \n",
      "Epoch:12 Learning rate 0.009484538757089937 Weight Decay 1e-07 Train Loss:0.9888903498649597  Valid Loss:1.210131287574768 \n",
      "Epoch:13 Learning rate 0.008345435193533538 Weight Decay 1e-07 Train Loss:0.9781292676925659  Valid Loss:1.1958396434783936 \n",
      "Epoch:14 Learning rate 0.007343139224077267 Weight Decay 1e-07 Train Loss:0.9788566827774048  Valid Loss:1.1985660791397095 \n",
      "Epoch:15 Learning rate 0.0064612201058086615 Weight Decay 1e-07 Train Loss:0.9796079397201538  Valid Loss:1.190683364868164 \n",
      "Epoch:16 Learning rate 0.005685220446157621 Weight Decay 1e-07 Train Loss:0.978527843952179  Valid Loss:1.203244924545288 \n",
      "Epoch:17 Learning rate 0.005002419201344232 Weight Decay 1e-07 Train Loss:0.9560608267784119  Valid Loss:1.1792274713516235 \n",
      "Epoch:18 Learning rate 0.004401623138974351 Weight Decay 1e-07 Train Loss:0.9803729057312012  Valid Loss:1.2040791511535645 \n",
      "Epoch:19 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:0.9699279069900513  Valid Loss:1.1975746154785156 \n",
      "Epoch:20 Learning rate 0.003407833775495654 Weight Decay 1e-07 Train Loss:0.9685156941413879  Valid Loss:1.2021933794021606 \n",
      "Epoch:21 Learning rate 0.002998549181158038 Weight Decay 1e-07 Train Loss:0.9549927115440369  Valid Loss:1.1904531717300415 \n",
      "Epoch:22 Learning rate 0.002638420117928374 Weight Decay 1e-07 Train Loss:0.9679828882217407  Valid Loss:1.202371597290039 \n",
      "Epoch:23 Learning rate 0.002321542952315606 Weight Decay 1e-07 Train Loss:0.9374255537986755  Valid Loss:1.174977421760559 \n",
      "Epoch:24 Learning rate 0.0020427230837210333 Weight Decay 1e-07 Train Loss:0.9418154954910278  Valid Loss:1.1808301210403442 \n",
      "Epoch:25 Learning rate 0.00179738978880607 Weight Decay 1e-07 Train Loss:0.9477411508560181  Valid Loss:1.1826475858688354 \n",
      "Epoch:26 Learning rate 0.001581521293145342 Weight Decay 1e-07 Train Loss:0.9652043581008911  Valid Loss:1.1901566982269287 \n",
      "Epoch:27 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.9494736194610596  Valid Loss:1.1816068887710571 \n",
      "Epoch:28 Learning rate 0.0012244486884222705 Weight Decay 1e-07 Train Loss:0.9408031105995178  Valid Loss:1.1830016374588013 \n",
      "Epoch:29 Learning rate 0.0010773910507136221 Weight Decay 1e-07 Train Loss:0.9342116117477417  Valid Loss:1.1732507944107056 \n",
      "Epoch:30 Learning rate 0.0009479951974577903 Weight Decay 1e-07 Train Loss:0.9451043009757996  Valid Loss:1.1869086027145386 \n",
      "Epoch:31 Learning rate 0.0008341399288659156 Weight Decay 1e-07 Train Loss:0.9278792142868042  Valid Loss:1.1744757890701294 \n",
      "Epoch:32 Learning rate 0.0007339588035828789 Weight Decay 1e-07 Train Loss:0.9592627286911011  Valid Loss:1.1972424983978271 \n",
      "Epoch:33 Learning rate 0.0006458095419184806 Weight Decay 1e-07 Train Loss:0.9456303715705872  Valid Loss:1.1788312196731567 \n",
      "Epoch:34 Learning rate 0.0005682471037842957 Weight Decay 1e-07 Train Loss:0.937997043132782  Valid Loss:1.1779636144638062 \n",
      "Epoch:35 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:0.960385799407959  Valid Loss:1.1964906454086304 \n",
      "Epoch:36 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.0722936391830444  Valid Loss:1.2494442462921143 \n",
      "Epoch:37 Learning rate 0.028140877874513245 Weight Decay 1e-07 Train Loss:1.106662631034851  Valid Loss:1.2763186693191528 \n",
      "Epoch:38 Learning rate 0.026396966918275637 Weight Decay 1e-07 Train Loss:1.0767055749893188  Valid Loss:1.2432875633239746 \n",
      "Epoch:39 Learning rate 0.0247611274101587 Weight Decay 1e-07 Train Loss:1.061600923538208  Valid Loss:1.2387888431549072 \n",
      "Epoch:40 Learning rate 0.023226662082817946 Weight Decay 1e-07 Train Loss:1.037764310836792  Valid Loss:1.2245025634765625 \n",
      "Epoch:41 Learning rate 0.021787288703505575 Weight Decay 1e-07 Train Loss:1.0302084684371948  Valid Loss:1.2253400087356567 \n",
      "Epoch:42 Learning rate 0.020437114354070415 Weight Decay 1e-07 Train Loss:1.0249799489974976  Valid Loss:1.2245454788208008 \n",
      "Epoch:43 Learning rate 0.01917061130484524 Weight Decay 1e-07 Train Loss:0.9962289333343506  Valid Loss:1.2013088464736938 \n",
      "Epoch:44 Learning rate 0.017982594383647098 Weight Decay 1e-07 Train Loss:1.0000168085098267  Valid Loss:1.203007698059082 \n",
      "Epoch:45 Learning rate 0.01686819974723736 Weight Decay 1e-07 Train Loss:0.9774379134178162  Valid Loss:1.1810234785079956 \n",
      "Epoch:46 Learning rate 0.015822864968330057 Weight Decay 1e-07 Train Loss:0.9864498376846313  Valid Loss:1.1972832679748535 \n",
      "Epoch:47 Learning rate 0.014842310356623001 Weight Decay 1e-07 Train Loss:0.9750863313674927  Valid Loss:1.1923270225524902 \n",
      "Epoch:48 Learning rate 0.013922521437378366 Weight Decay 1e-07 Train Loss:0.9679083228111267  Valid Loss:1.181549072265625 \n",
      "Epoch:49 Learning rate 0.013059732515818573 Weight Decay 1e-07 Train Loss:0.9567088484764099  Valid Loss:1.1794639825820923 \n",
      "Epoch:50 Learning rate 0.01225041126004867 Weight Decay 1e-07 Train Loss:0.9639766216278076  Valid Loss:1.1814557313919067 \n",
      "Epoch:51 Learning rate 0.011491244239386383 Weight Decay 1e-07 Train Loss:0.9582192897796631  Valid Loss:1.1816856861114502 \n",
      "Epoch:52 Learning rate 0.010779123358892535 Weight Decay 1e-07 Train Loss:0.9357274174690247  Valid Loss:1.1661783456802368 \n",
      "Epoch:53 Learning rate 0.010111133134563594 Weight Decay 1e-07 Train Loss:0.951348066329956  Valid Loss:1.1812649965286255 \n",
      "Epoch:54 Learning rate 0.009484538757089947 Weight Decay 1e-07 Train Loss:0.9594206809997559  Valid Loss:1.1840708255767822 \n",
      "Epoch:55 Learning rate 0.00889677489531186 Weight Decay 1e-07 Train Loss:0.9436328411102295  Valid Loss:1.1819003820419312 \n",
      "Epoch:56 Learning rate 0.008345435193533549 Weight Decay 1e-07 Train Loss:0.945691704750061  Valid Loss:1.1777414083480835 \n",
      "Epoch:57 Learning rate 0.007828262419696414 Weight Decay 1e-07 Train Loss:0.9475082755088806  Valid Loss:1.1792608499526978 \n",
      "Epoch:58 Learning rate 0.007343139224077278 Weight Decay 1e-07 Train Loss:0.9273214340209961  Valid Loss:1.16615629196167 \n",
      "Epoch:59 Learning rate 0.006888079470676887 Weight Decay 1e-07 Train Loss:0.9313546419143677  Valid Loss:1.1759191751480103 \n",
      "Epoch:60 Learning rate 0.006461220105808671 Weight Decay 1e-07 Train Loss:0.9416384696960449  Valid Loss:1.1782100200653076 \n",
      "Epoch:61 Learning rate 0.006060813530597045 Weight Decay 1e-07 Train Loss:0.9203944802284241  Valid Loss:1.1674622297286987 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,None,62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.7706148624420166  Valid Loss:1.2679154872894287 \n",
      "Epoch:1 Learning rate 0.0038729833462074164 Weight Decay 1e-07 Train Loss:1.151675820350647  Valid Loss:1.1820484399795532 \n",
      "Epoch:2 Learning rate 0.0004999999999999999 Weight Decay 1e-07 Train Loss:1.087894082069397  Valid Loss:1.1778688430786133 \n",
      "Epoch:3 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.1772061586380005  Valid Loss:1.2358553409576416 \n",
      "Epoch:4 Learning rate 0.010779123358892525 Weight Decay 1e-07 Train Loss:1.0867831707000732  Valid Loss:1.159386157989502 \n",
      "Epoch:5 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:1.030679702758789  Valid Loss:1.1434040069580078 \n",
      "Epoch:6 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:1.0042517185211182  Valid Loss:1.141108512878418 \n",
      "Epoch:7 Learning rate 0.0004999999999999998 Weight Decay 1e-07 Train Loss:1.0106353759765625  Valid Loss:1.1401408910751343 \n",
      "Epoch:8 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.104586124420166  Valid Loss:1.2386051416397095 \n",
      "Epoch:9 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:1.1217234134674072  Valid Loss:1.1834478378295898 \n",
      "Epoch:10 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:1.048125982284546  Valid Loss:1.1890671253204346 \n",
      "Epoch:11 Learning rate 0.006461220105808663 Weight Decay 1e-07 Train Loss:1.026564598083496  Valid Loss:1.1591825485229492 \n",
      "Epoch:12 Learning rate 0.0038729833462074173 Weight Decay 1e-07 Train Loss:1.0443110466003418  Valid Loss:1.1483303308486938 \n",
      "Epoch:13 Learning rate 0.0023215429523156072 Weight Decay 1e-07 Train Loss:1.0069389343261719  Valid Loss:1.1501199007034302 \n",
      "Epoch:14 Learning rate 0.0013915788418568708 Weight Decay 1e-07 Train Loss:0.9969035387039185  Valid Loss:1.1406852006912231 \n",
      "Epoch:15 Learning rate 0.0008341399288659162 Weight Decay 1e-07 Train Loss:0.9843985438346863  Valid Loss:1.1401373147964478 \n",
      "Epoch:16 Learning rate 0.0005000000000000002 Weight Decay 1e-07 Train Loss:1.0071152448654175  Valid Loss:1.1400076150894165 \n",
      "Epoch:17 Learning rate 0.03 Weight Decay 1e-07 Train Loss:1.0977407693862915  Valid Loss:1.244201898574829 \n",
      "Epoch:18 Learning rate 0.02322666208281794 Weight Decay 1e-07 Train Loss:1.1214226484298706  Valid Loss:1.1993082761764526 \n",
      "Epoch:19 Learning rate 0.017982594383647087 Weight Decay 1e-07 Train Loss:1.1146548986434937  Valid Loss:1.2180440425872803 \n",
      "Epoch:20 Learning rate 0.013922521437378356 Weight Decay 1e-07 Train Loss:1.0590180158615112  Valid Loss:1.1533461809158325 \n",
      "Epoch:21 Learning rate 0.010779123358892527 Weight Decay 1e-07 Train Loss:1.0259778499603271  Valid Loss:1.1543872356414795 \n",
      "Epoch:22 Learning rate 0.00834543519353354 Weight Decay 1e-07 Train Loss:1.0275230407714844  Valid Loss:1.150193452835083 \n",
      "Epoch:23 Learning rate 0.006461220105808662 Weight Decay 1e-07 Train Loss:0.9908444881439209  Valid Loss:1.1456794738769531 \n",
      "Epoch:24 Learning rate 0.005002419201344232 Weight Decay 1e-07 Train Loss:0.9823666214942932  Valid Loss:1.1510065793991089 \n",
      "Epoch:25 Learning rate 0.003872983346207416 Weight Decay 1e-07 Train Loss:0.977562427520752  Valid Loss:1.1477148532867432 \n",
      "Epoch:26 Learning rate 0.002998549181158038 Weight Decay 1e-07 Train Loss:0.9675324559211731  Valid Loss:1.1474310159683228 \n",
      "Epoch:27 Learning rate 0.002321542952315606 Weight Decay 1e-07 Train Loss:0.9873231053352356  Valid Loss:1.1411833763122559 \n",
      "Epoch:28 Learning rate 0.00179738978880607 Weight Decay 1e-07 Train Loss:0.9468125104904175  Valid Loss:1.1422600746154785 \n",
      "Epoch:29 Learning rate 0.0013915788418568699 Weight Decay 1e-07 Train Loss:0.9670811891555786  Valid Loss:1.1417224407196045 \n",
      "Epoch:30 Learning rate 0.0010773910507136221 Weight Decay 1e-07 Train Loss:0.9552481174468994  Valid Loss:1.1417756080627441 \n"
     ]
    }
   ],
   "source": [
    "learner.run_epochs(dltrain,dlvalid,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 9724])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[0][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder[1][0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mtx_1_weights=autoenc.encoder[0][0].weight.data.cpu().numpy()\n",
    "user_mtx_2_weights=autoenc.encoder[1][0].weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([user_mtx_1_weights,user_mtx_2_weights],open(f'{DATAPATH}/inter/user_autoenc_weights.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=9724, out_features=20, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.25)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.25)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
